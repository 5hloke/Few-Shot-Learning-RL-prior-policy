{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import sys\n",
    "import torch\n",
    "\n",
    "DATASETS = '../datasets/'\n",
    "\n",
    "def read_file(dir_name):\n",
    "    main_df = pd.DataFrame()\n",
    "    directory = DATASETS + dir_name\n",
    "    for filename in os.listdir(directory):\n",
    "        data = np.load(os.path.join(directory, filename))\n",
    "        data_dict = {}\n",
    "        for keys in data.keys():\n",
    "            data_dict[keys] = list(data[keys])\n",
    "        df = pd.DataFrame.from_dict(data_dict)\n",
    "        main_df = pd.concat([main_df, df])\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mw/cls_sweep-v2\n",
      "mw/cls_push-v2\n",
      "mw/cls_door-open-v2\n",
      "mw/cls_peg-insert-side-v2\n",
      "mw/cls_drawer-close-v2\n",
      "mw/cls_basketball-v2\n",
      "mw/cls_reach-v2\n",
      "mw/cls_window-open-v2\n",
      "mw/cls_pick-place-v2\n",
      "mw/cls_button_press_topdown-v2\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for name in os.listdir(\"../datasets/mw\"):\n",
    "    inner_df = pd.DataFrame()\n",
    "    if not (name.startswith('.')):\n",
    "        dir_name = 'mw/'+name\n",
    "        print(dir_name)\n",
    "        df = read_file(dir_name)\n",
    "        inner_df = pd.concat([inner_df, df])    \n",
    "    data.append(inner_df)\n",
    "data = np.array(data, dtype=object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class MetaRewardMLPEnsemble(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layers=[256, 256, 256],\n",
    "        # ensemble_size=3,\n",
    "        act=F.leaky_relu,\n",
    "        output_act=torch.tanh,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        params = {}\n",
    "        last_dim = 39 + 4\n",
    "        self.num_layers = len(hidden_layers) + 1\n",
    "        for i, dim in enumerate(\n",
    "            hidden_layers\n",
    "            + [\n",
    "                1,\n",
    "            ]\n",
    "        ):\n",
    "            weight = torch.empty(last_dim, dim)\n",
    "            weight.transpose_(0, 1)\n",
    "            nn.init.kaiming_uniform_(weight, a=math.sqrt(5))\n",
    "            weight.transpose_(0, 1)\n",
    "            params[f\"linear_w_{i}\"] = nn.Parameter(weight)\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(params[f\"linear_w_{i}\"].T)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            params[f\"linear_b_{i}\"] = nn.Parameter(\n",
    "                nn.init.uniform_(torch.empty(1, dim, requires_grad=True), -bound, bound)\n",
    "            )\n",
    "            last_dim = dim\n",
    "\n",
    "        self.params = nn.ParameterDict(params)\n",
    "        self.parameters = self.params.parameters ######### NEW\n",
    "        # self.ensemble_size = ensemble_size\n",
    "        self.act = act\n",
    "        self.output_act = output_act\n",
    "\n",
    "    def forward(self, x, params=None):\n",
    "        if params is None:\n",
    "            params = self.params\n",
    "        # x = torch.cat((obs, action), dim=1)\n",
    "        # x = x.repeat(self.ensemble_size, 1, 1)\n",
    "        for i in range(self.num_layers):\n",
    "            # print('b_0: ', params[f\"linear_b_{i}\"].repeat(n_x, 1).shape)\n",
    "            # print('x:', x.shape)\n",
    "            # print('w_0: ', params['linear_w_0'].shape)\n",
    "            # x = torch.baddbmm(params[f\"linear_b_{i}\"], x, params[f\"linear_w_{i}\"])\n",
    "            # print(params)\n",
    "            x = params[f\"linear_b_{i}\"] + torch.matmul( x, params[f\"linear_w_{i}\"] )\n",
    "            if i == self.num_layers - 1:\n",
    "                x = self.output_act(x)\n",
    "            else:\n",
    "                x = self.act(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size1=256, hidden_size2=256, hidden_size3=256, output_size = 2750):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "#         self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "#         self.out = nn.Linear(hidden_size2, 1)\n",
    "    \n",
    "#     # def forward_inner(self, x, parameters):\n",
    "#     #     x = torch.relu(self.fc1(x, parameters['fc1.weight'], parameters['fc1.bias']))\n",
    "#     #     x = torch.relu(self.fc2(x, parameters['fc2.weight'], parameters['fc2.bias']))\n",
    "#     #     x = torch.tanh(self.out(x, parameters['out.weight'], parameters['out.bias']))\n",
    "#     #     return x\n",
    "#     def forward(self, x):\n",
    "#         # print(x.shape)\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.tanh(self.out(x))\n",
    "#         return x\n",
    "    \n",
    "#     def para_forw(self, x, param):\n",
    "#         # print(param['fc1.weight'].shape)\n",
    "#         x = torch.relu(nn.functional.linear(x, param['fc1.weight'], param['fc1.bias']))\n",
    "#         x = torch.relu(nn.functional.linear(x, param['fc2.weight'], param['fc2.bias']))\n",
    "#         x = torch.tanh(nn.functional.linear(x, param['out.weight'], param['out.bias']))\n",
    "        # return x\n",
    "\n",
    "\n",
    "class PreferenceMAML:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ml10,\n",
    "        input_size,\n",
    "        hidden_size1,\n",
    "        hidden_size2,\n",
    "        outer_lr = 1e-7,\n",
    "        inner_lr = 0.001,\n",
    "        num_support=10,\n",
    "        num_query=10,\n",
    "        num_inner_steps=5,\n",
    "        k = 25,\n",
    "        num_tasks = 10,\n",
    "        episode_per_task = 1250,\n",
    "        output_size = 2750,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.ml10 = ml10\n",
    "        self.reward_criterion =  nn.BCELoss()\n",
    "        self.num_support = num_support\n",
    "        self.num_query = num_query\n",
    "        self.num_inner_steps = num_inner_steps\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        self.k = k\n",
    "        self.num_tasks = num_tasks\n",
    "        self.episode_per_task = episode_per_task\n",
    "        self.num_segments = None\n",
    "        self.model = MetaRewardMLPEnsemble()\n",
    "\n",
    "\n",
    "    def construct_episodes(self, ml10):\n",
    "        # episodes - n (tasks) x num_episode (each npz file)  , each cell is a dataframe of the episode \n",
    "        self.episodes = []\n",
    "        for task in ml10:\n",
    "            if(len(task)==0):\n",
    "                continue\n",
    "            task_episodes=[]\n",
    "            row_index = task[task['done'] == True].index.tolist()\n",
    "            prev=0\n",
    "            for x in row_index:\n",
    "                task_episodes.append(task[prev:x+1])\n",
    "                prev=x+1\n",
    "            task_episodes = np.array(task_episodes,dtype=object)\n",
    "            self.episodes.append(task_episodes)\n",
    "        self.episodes = np.array(self.episodes,dtype=object)\n",
    "        # return episodes\n",
    "\n",
    "    def form_sigma_groups(self, episode):\n",
    "        #num_segments = int(episode.shape[0] / self.k)\n",
    "        split_indices = np.arange(self.k, episode.shape[0], self.k)\n",
    "        # print(num_segments)\n",
    "        if len(split_indices) != 0:\n",
    "            l_segment = np.array_split(episode.iloc[::-1][:(self.k*(episode.shape[0] // self.k))], split_indices)\n",
    "            for i in range(len(l_segment)):\n",
    "                l_segment[i] = l_segment[i].iloc[::-1]\n",
    "            if(len(l_segment[-1])<25):\n",
    "                l_segment=l_segment[:-1]\n",
    "            return l_segment\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def compare_probabilities(self, sigma1, sigma2):\n",
    "        exp_sum_rewards_sigma1 = np.exp(sum(row['reward'] for row in sigma1))\n",
    "        exp_sum_rewards_sigma2 = np.exp(sum(row['reward'] for row in sigma2))\n",
    "        prob = exp_sum_rewards_sigma1 / (exp_sum_rewards_sigma1 + exp_sum_rewards_sigma2)\n",
    "        return [0] if prob > 0.5 else [1]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        X = []\n",
    "        y = []\n",
    "        episodes = self.episodes\n",
    "        # sigmas = self.form_sigma_groups(episodes, k)\n",
    "        sigmas = []\n",
    "        for task in episodes:\n",
    "            sigma = []\n",
    "            for episode in task:\n",
    "                segment = self.form_sigma_groups(episode)\n",
    "                # print(len(segment))\n",
    "                if segment is not None:\n",
    "                    sigma.append(segment)\n",
    "            # sigma = [self.form_sigma_groups(episode, k) for episode in task]\n",
    "            sigmas.append(sigma)\n",
    "        sigmas = np.array(sigmas, dtype=object)\n",
    "        all_lengths = [len(episode) for task in sigmas for episode in task]\n",
    "        self.num_segments = min(all_lengths)\n",
    "        if(self.num_segments<3):\n",
    "            self.num_segments=3\n",
    "        for task in sigmas:\n",
    "            task_list=[]\n",
    "            for episode in task:\n",
    "                ep_list=[]\n",
    "                if(len(episode)<self.num_segments):\n",
    "                    continue\n",
    "                for i in range(self.num_segments):\n",
    "                    y.append(episode[i][\"reward\"])\n",
    "                    ep_list.append(episode[i].drop('reward', axis=1))\n",
    "                task_list.append(ep_list)\n",
    "            task_list=np.array(task_list, dtype=object).reshape(-1,1) # convert row vector to col vector\n",
    "            X.append(task_list)\n",
    "        X_new=[]\n",
    "        task_counter = []\n",
    "        for task in X:\n",
    "            counter = 0\n",
    "            for i in range(0,len(task),4):\n",
    "                X_new.append(np.concatenate((task[i][0], task[i+1][0])))\n",
    "                counter += 1\n",
    "            task_counter.append(counter)\n",
    "        X = np.array(X_new, dtype=object)\n",
    "        task_counts = np.array(task_counter, dtype=object)\n",
    "        y=np.array(y,dtype=object)\n",
    "        y=y.flatten()\n",
    "        return X, y, task_counts\n",
    "    \n",
    "    def batchify(self, X, y, task_lengths, task_no, num=110):\n",
    "        # print(task_no)\n",
    "        if task_no == 0:\n",
    "            task_beg = 0  \n",
    "            task_end = task_lengths[0]\n",
    "        else:    \n",
    "            task_beg = np.sum(task_lengths[0:task_no])    \n",
    "            task_end = np.sum(task_lengths[0:task_no+1])\n",
    "        rand = random.sample(range(0,400), 110)\n",
    "        X_task = X[task_beg:task_end]\n",
    "        y_task = y[task_beg:task_end]\n",
    "        X_random = []\n",
    "        y_random = []\n",
    "        for r in rand:\n",
    "            X_random.append(X_task[r*25 : r*25+25])\n",
    "            y_random.append(y_task[r*25 : r*25+25])\n",
    "        X_random = np.array(X_random, dtype=np.float32)\n",
    "        y_random = np.array(y_random, dtype=np.float32)\n",
    "        return X_random.reshape(2750,43), y_random.reshape(2750,)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, y, task_lengths, num_epochs):\n",
    "        # optimizer = optim.Adam(self.model.params.values(), lr = self.outer_lr) ######### self.model.parameters() self.model.params.values()\n",
    "        # optimizer.zero_grad()\n",
    "        # self.optim[\"reward\"].zero_grad()\n",
    "        # print(self.model.state_dict())\n",
    "        # print(self.model.parameters())\n",
    "        plt_x = np.arange(0, num_epochs)\n",
    "        plt_y = np.zeros(num_epochs)\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            # optimizer.zero_grad()\n",
    "            loss = self._outer_step(self.model, X, y, task_lengths) \n",
    "            if loss is None:\n",
    "                return {}\n",
    "            print('loss from train:', loss)\n",
    "            plt_y[epoch] = loss\n",
    "            # print(\"Train Loss 1: \",plt_y[epoch])\n",
    "            # print(\"Train loss 2: \", loss)\n",
    "\n",
    "            #######################################\n",
    "            #######################################\n",
    "\n",
    "            for k, val in self.model.params.items():\n",
    "                print(k)\n",
    "                print(val)\n",
    "            \n",
    "            grads = torch.autograd.grad(loss, self.model.params.values(), create_graph=True)\n",
    "\n",
    "            # print(grads) ####### 2nd epoch onwards all 0. EVEN AFTER reducing the outer_lr to 1e-7\n",
    "\n",
    "            idx = 0\n",
    "            # new_dict = {}\n",
    "            for name, w in self.model.params.items():\n",
    "                self.model.params[name] = self.model.params[name] - self.outer_lr * grads[idx][0]\n",
    "                # raise Exception\n",
    "                idx += 1\n",
    "                # w.copy_(w_prime)\n",
    "                # new_dict[name] = w_prime\n",
    "            # return new_dict\n",
    "            #######################################\n",
    "            #######################################\n",
    "            \n",
    "\n",
    "            # if epoch%5 == 0:\n",
    "            #     plt.plot(plt_x[:epoch], plt_y[:epoch])\n",
    "            #     plt.xlabel(\"Epochs\")\n",
    "            #     plt.ylabel(\"Mean Loss\")\n",
    "            #     plt.show() \n",
    "            #     print(f\"Epoch: {epoch}, Loss: {plt_y[epoch]}\")\n",
    "            \n",
    "            # print(loss.grad)\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            # self.optim[\"reward\"].step()\n",
    "\n",
    "\n",
    "    def _outer_step(self, model, X, y, task_lengths):\n",
    "        outer_losses = []\n",
    "        for t in range(self.num_tasks):\n",
    "            X_b, y_b = self.batchify(X, y, task_lengths, t)\n",
    "            H, W = X_b.shape\n",
    "            inner_x = X_b[:H//2, :]\n",
    "            inner_y = y_b[:H//2]\n",
    "            outer_x = X_b[H//2:, :]\n",
    "            outer_y = y_b[H//2:]\n",
    "            weights = self._inner_loop(inner_x, inner_y, model = model)\n",
    "            # model_outer = copy.deepcopy(model)\n",
    "            # model_outer = Model(43, 256, 256, 256)\n",
    "            # model_outer.load_state_dict(weights)\n",
    "            # model_outer.load_state_dict(weights)\n",
    "            outer_losses.append(self._compute_loss(outer_x, outer_y, model, parameters=weights))\n",
    "        # print(outer_losses)\n",
    "        if (len(outer_losses) == 0):\n",
    "            return None\n",
    "        outer_loss = torch.mean(torch.stack(outer_losses))\n",
    "        # print(\"Outer Loss: \", loss)\n",
    "        return outer_loss\n",
    "\n",
    "\n",
    "            \n",
    "    def _inner_loop(self, X, y, model):\n",
    "        # parameters = {k: torch.clone(v) for k, v in model.ParameterDict()}\n",
    "        # model_inner = copy.deepcopy(model)\n",
    "        # opt_inner = optim.SGD(model_inner.parameters(), lr = self.inner_lr)\n",
    "        # state_dict = model.state_dict()\n",
    "        # print(id(model.parameters()))\n",
    "        # print(id(model.params))\n",
    "        # print(model.state_dict())\n",
    "        params_og = model.params\n",
    "        new_dict = {k: torch.clone(v) for k, v in params_og.items()} ######### state_dict\n",
    "        \n",
    "\n",
    "        # print(new_dict)                                                        ######### IN 2nd EPOCH SOMEHOW new_dict has values {new_dict from 1st epoch +/- 256}\n",
    "\n",
    "\n",
    "        loss = self._compute_loss(X, y, model, parameters = new_dict)\n",
    "        # print(\"Loss computed\")\n",
    "        # grad = torch.autograd.grad(loss, parameters, allow_unused=True)\n",
    "        # for name, w in model_inner.named_parameters():\n",
    "        #     print(name, w)\n",
    "        # print('loss from inner:',loss)\n",
    "\n",
    "        grad = torch.autograd.grad(loss, new_dict.values(), create_graph=True)\n",
    "\n",
    "        # grad = []\n",
    "        # # print(new_dict.values().requires_grad)\n",
    "        # for val in new_dict.values():\n",
    "        #     # print(val)\n",
    "        #     # val.requires_grad = True\n",
    "        #     # print(val.requires_grad)\n",
    "        #     grad.append(torch.autograd.grad(loss, val, create_graph=True))\n",
    "            # print(\"Gradient\")\n",
    "            # print(grad)\n",
    "            # raise Exception\n",
    "        # print(\"Grad: \")\n",
    "        # print(grad)\n",
    "        # print(len(grad))\n",
    "        # raise Exception\n",
    "        # print(len(grad))\n",
    "        \n",
    "        # print(grad)\n",
    "        idx = 0\n",
    "        # new_dict = {}\n",
    "        for name, w in params_og.items():\n",
    "            # print(grad[idx][0])\n",
    "            new_dict[name] = new_dict[name] - self.inner_lr * grad[idx][0]\n",
    "            # raise Exception\n",
    "            idx += 1\n",
    "            # w.copy_(w_prime)\n",
    "            # new_dict[name] = w_prime\n",
    "        return new_dict\n",
    "        # for name, w in model_inner.named_parameters():\n",
    "        #     print(name, w)\n",
    "        # raise Exception\n",
    "        # weights = list(map(lambda p: p[1] - self.inner_lr * p[0], zip(grad, parameters)))\n",
    "        # print(weights)\n",
    "\n",
    "        # print(\"Inner loss: \", loss)\n",
    "        # opt_inner.zero_grad()\n",
    "        # loss.backward()\n",
    "        \n",
    "        # opt_inner.step()\n",
    "\n",
    "        # grads = torch.autograd.grad(loss, parameters, create_graph=True)\n",
    "        #for j,k in enumerate(parameters.keys()):\n",
    "         #   parameters[k] = parameters[k] - self.inner_lr * grads[j]\n",
    "        # for name, w in model_inner.named_parameters():\n",
    "        #     if 'weight' in name:\n",
    "        #         w = w - self.inner_lr * grads\n",
    "\n",
    "        # parameters = parameters - self.inner_lr * grads\n",
    "        # return weights\n",
    "\n",
    "    '''\n",
    "    R_E = \n",
    "    [\n",
    "        [\n",
    "            sigma_s1^E1, ..., sigma_s#^E1\n",
    "        ],\n",
    "        [\n",
    "            sigma_s1^E2, ..., sigma_s#^E2\n",
    "        ], ...,\n",
    "        [\n",
    "            sigma_s1^EN, ..., sigma_s#^EN\n",
    "        ]\n",
    "    ]\n",
    "    '''\n",
    "    #self.k = segment lenght\n",
    "    def _compute_loss(self, X, y, model, parameters = None):\n",
    "        # print(X, y, X.shape, y.shape)\n",
    "        # state_dict = model.state_dict()\n",
    "        # if parameters is not None:\n",
    "        #     model.load_state_dict(parameters)\n",
    "        \n",
    "        X_tensor = torch.from_numpy(X)\n",
    "        y_tensor = torch.from_numpy(y)\n",
    "        if parameters is not None:\n",
    "            output_reward = model.forward(X_tensor, parameters) # WORKING\n",
    "        else:\n",
    "            print('Params are NONE')\n",
    "            output_reward = model(X_tensor)\n",
    "        # N_o, _ = output_reward.shape\n",
    "        # print(output_reward.shape)\n",
    "        x =  y_tensor.shape[0]\n",
    "        N = x//self.k\n",
    "        output_reward = output_reward.reshape(N, self.k)\n",
    "        output_reward = torch.sum(output_reward, dim=1)\n",
    "        y_tensor = y_tensor.reshape(N, self.k)\n",
    "        y_tensor = torch.sum(y_tensor, dim=1)\n",
    "        loss = 0\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        #out_mat = np.exp(out_mat)\n",
    "        # out_logit = []\n",
    "        # out_y = []\n",
    "        loss = []\n",
    "        for i in range(N):\n",
    "            # sig_1 = output_reward[i]\n",
    "            # y_1 = y_tensor[i]\n",
    "            for j in range(i+1, N):\n",
    "                # sig_2 = output_reward[j]\n",
    "                # y_2 = y_tensor[j]\n",
    "                if y_tensor[i] > y_tensor[j]:\n",
    "                    loss.append(criterion(output_reward[j] - output_reward[i], torch.tensor(0.0, requires_grad= True)))\n",
    "                else:\n",
    "                    loss.append(criterion(output_reward[j] - output_reward[i], torch.tensor(1.0, requires_grad= True)))\n",
    "\n",
    "        loss = torch.sum(torch.stack(loss))\n",
    "        # if parameters is not None:\n",
    "        #     model.load_state_dict(state_dict)\n",
    "        return loss\n",
    " # for i in range(N):\n",
    "        #     # print(torch.sum(output_reward[i*self.k:(i+1)*self.k]).detach().numpy())\n",
    "        #     out_mat[i, :] = torch.sum(output_reward[i*self.k:(i+1)*self.k]).detach().numpy()\n",
    "        #     y_mat[i, :] = torch.sum(y_tensor[i*self.k:(i+1)*self.k]).detach().numpy()\n",
    "  # if (model is not None) and (parameters is not None) :\n",
    "        #     print(\"Errors\")\n",
    "        #     return\n",
    "        # if model is not None:  \n",
    "        #     output_reward = model(X_tensor)\n",
    "        # elif parameters is not None:\n",
    "        #     output_reward = self.model(X_tensor, parameters)\n",
    "        # else:\n",
    "        #     output_reward = self.model(X_tensor)\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "\n",
    "ml10 = data.copy()  \n",
    "input_size = 43  # Assuming obs has 39 numbers and action has 4 numbers * 2 for pair of sigmas\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 256\n",
    "hidden_size3 = 256\n",
    "# hidden_size3 = 1024\n",
    "# hidden_size4 = 2075\n",
    "\n",
    "output_size = 1\n",
    "num_epochs = 5\n",
    "outer_lr = 0.0001\n",
    "\n",
    "model = PreferenceMAML(ml10, input_size, hidden_size1, hidden_size2, hidden_size3)\n",
    "# model.setup_optimizers(optim.Adam, {\"lr\": outer_lr})\n",
    "\n",
    "model.construct_episodes(ml10)\n",
    "print('Preparing Data.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, task_lengths = model.prepare_data()\n",
    "print('Data Preparation Done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from train: tensor(1260.4908, grad_fn=<MeanBackward0>)\n",
      "linear_b_0\n",
      "Parameter containing:\n",
      "tensor([[-0.1149,  0.0721,  0.0852,  0.1292,  0.1072,  0.1445,  0.0197,  0.0042,\n",
      "          0.1222, -0.0241, -0.0569, -0.0800, -0.1121, -0.0752, -0.0449,  0.1410,\n",
      "          0.0953, -0.0102,  0.1419,  0.1441, -0.1503,  0.0902, -0.0716, -0.1311,\n",
      "         -0.0092,  0.1358,  0.0151,  0.0694,  0.0862, -0.0527,  0.1371, -0.1156,\n",
      "          0.0993, -0.0577,  0.0865,  0.0170, -0.1076,  0.0265, -0.0252, -0.0569,\n",
      "          0.1234, -0.1417, -0.1511,  0.0694,  0.0518,  0.1115, -0.0024,  0.1151,\n",
      "         -0.1071,  0.1501,  0.0532, -0.0988,  0.0720, -0.1408, -0.0914, -0.1253,\n",
      "          0.0934,  0.0281,  0.0250,  0.1191, -0.1363,  0.0942, -0.0085, -0.1362,\n",
      "         -0.1460, -0.0669,  0.0695, -0.0261, -0.0905, -0.1196,  0.0113,  0.1051,\n",
      "         -0.0245,  0.1248, -0.0087,  0.0929, -0.1443,  0.0538,  0.1390, -0.0248,\n",
      "          0.1511,  0.1188, -0.0459, -0.1019,  0.0206,  0.0208, -0.1265, -0.0730,\n",
      "         -0.0692, -0.0652, -0.0332,  0.1457, -0.0208,  0.0061, -0.0118,  0.0207,\n",
      "          0.0654,  0.1065,  0.0222,  0.0630,  0.0566, -0.0653,  0.0098,  0.1482,\n",
      "          0.1459, -0.0486, -0.0035,  0.0168, -0.0712, -0.1023, -0.0911,  0.0414,\n",
      "          0.0503,  0.0652, -0.0096,  0.0660, -0.1229,  0.0666, -0.0378, -0.1015,\n",
      "          0.0023, -0.0691, -0.0866, -0.0213, -0.0199,  0.1159, -0.1135, -0.0714,\n",
      "          0.1499,  0.1477,  0.1210,  0.1307, -0.0141,  0.0619, -0.1119,  0.1156,\n",
      "         -0.0554,  0.0739, -0.0474, -0.0076,  0.1274, -0.1380,  0.0247,  0.0947,\n",
      "         -0.0354, -0.1183,  0.0403,  0.0123,  0.0937, -0.1325,  0.0986, -0.1494,\n",
      "         -0.0436,  0.0026, -0.1106,  0.0162, -0.0267, -0.0615, -0.0813,  0.0500,\n",
      "          0.0354,  0.0764,  0.0329,  0.0889, -0.1269,  0.0578, -0.0962,  0.0324,\n",
      "         -0.0895, -0.0121, -0.0679, -0.0019,  0.0022, -0.0824, -0.1438,  0.0524,\n",
      "          0.1044,  0.0187,  0.1185, -0.0602, -0.1182,  0.1307, -0.0634,  0.1519,\n",
      "         -0.0207, -0.0677,  0.1508, -0.0682, -0.0591,  0.1398, -0.1465,  0.0645,\n",
      "          0.0344, -0.1227, -0.0596,  0.1300,  0.1226, -0.1308,  0.0667,  0.1280,\n",
      "          0.0308,  0.0440, -0.0096, -0.0300,  0.1265,  0.1406,  0.0673, -0.0940,\n",
      "          0.1308, -0.1396, -0.0812, -0.0763,  0.1169,  0.0807, -0.0510,  0.0153,\n",
      "         -0.0517,  0.0817, -0.1091, -0.0852, -0.0984, -0.0747,  0.0771, -0.1035,\n",
      "          0.0661, -0.0339, -0.0570,  0.0983, -0.0336, -0.0086,  0.0304, -0.0792,\n",
      "         -0.0271, -0.1327, -0.1176,  0.0261,  0.1498,  0.1155, -0.1413, -0.0263,\n",
      "          0.0354, -0.0014,  0.0996, -0.0426,  0.0533,  0.0073, -0.0805,  0.0779,\n",
      "          0.1467, -0.1246, -0.0373,  0.0510,  0.1059, -0.1069,  0.0902, -0.0079]],\n",
      "       requires_grad=True)\n",
      "linear_b_1\n",
      "Parameter containing:\n",
      "tensor([[-2.5874e-02,  2.1904e-02,  3.7966e-02,  2.9528e-02,  5.7015e-02,\n",
      "         -5.7842e-03, -6.1646e-02, -1.2199e-02, -2.3125e-02, -4.0860e-02,\n",
      "         -5.5533e-02, -6.0079e-02, -5.2799e-02,  2.3181e-03,  5.6205e-02,\n",
      "         -3.8146e-02,  2.9635e-02, -2.3228e-02,  2.7050e-02,  1.1306e-02,\n",
      "         -4.1684e-03,  3.7711e-02,  5.0705e-02, -3.5987e-03, -3.6802e-02,\n",
      "          3.0907e-02, -1.8564e-02, -4.1024e-02, -4.5940e-05, -5.7581e-02,\n",
      "         -5.1450e-02, -6.1540e-02, -5.5359e-02,  1.9264e-02, -9.6396e-03,\n",
      "          5.5875e-02,  2.8676e-02,  5.3928e-02,  3.7821e-02, -3.3226e-02,\n",
      "          3.0160e-02,  6.1324e-02, -4.0071e-02, -2.9133e-02,  9.8392e-03,\n",
      "         -2.7325e-02, -1.1754e-02,  5.6597e-02, -3.3075e-02,  4.5647e-02,\n",
      "          2.4946e-02,  3.4577e-02, -2.6958e-02, -5.8795e-02, -1.2686e-02,\n",
      "          6.6224e-03,  5.8981e-03,  2.7566e-02, -3.3342e-02, -2.6768e-03,\n",
      "         -1.1268e-02, -1.1769e-02,  2.6512e-02, -2.7988e-02, -5.9247e-03,\n",
      "         -3.1137e-02, -6.4443e-03,  1.5123e-02, -1.0325e-02, -1.8736e-02,\n",
      "         -6.1582e-03,  3.4058e-02,  5.9496e-02, -1.1204e-02,  5.3603e-02,\n",
      "          1.8048e-02, -1.9928e-02, -6.1547e-03, -4.3000e-02, -5.0085e-03,\n",
      "         -5.8520e-02,  1.0819e-02,  3.0482e-02, -2.0192e-03, -1.6855e-02,\n",
      "          3.1419e-02, -1.0436e-02, -6.3288e-03, -3.0293e-02,  1.1477e-02,\n",
      "         -1.1752e-02,  2.8236e-02,  5.9564e-02, -1.1760e-02,  3.1657e-02,\n",
      "         -2.3519e-02, -6.1381e-03,  2.8921e-02,  2.7139e-02,  3.0101e-02,\n",
      "         -3.7172e-02, -1.1813e-03,  3.4192e-02,  1.8083e-03,  1.4396e-02,\n",
      "         -5.1177e-02, -5.7442e-02, -5.4745e-02,  1.4861e-02,  2.4872e-02,\n",
      "         -4.5240e-02, -3.5660e-02,  6.8482e-03, -5.4383e-02,  1.0079e-02,\n",
      "         -6.0511e-02, -4.2155e-02,  3.8397e-02, -3.2695e-02, -2.5086e-02,\n",
      "         -5.1258e-02,  2.3345e-02,  3.4278e-03, -5.6783e-02,  4.0105e-02,\n",
      "         -8.4583e-03, -3.5510e-02,  4.7343e-02,  4.4463e-02, -5.2862e-02,\n",
      "         -1.2732e-02, -5.5266e-02,  4.1172e-02,  3.6215e-02, -5.3548e-02,\n",
      "         -1.9306e-03, -4.8119e-02,  3.5804e-02, -2.8955e-03,  2.7979e-03,\n",
      "         -7.6268e-03, -3.9360e-02,  1.1859e-02, -3.1874e-02,  1.2413e-02,\n",
      "          2.1640e-02, -4.9648e-02, -2.8725e-02, -4.3419e-02, -1.2942e-04,\n",
      "         -1.2818e-02,  2.0978e-02, -1.3499e-02, -4.8214e-02,  4.4733e-02,\n",
      "          1.3489e-02, -4.0048e-02, -4.1638e-02,  2.7231e-02,  1.9659e-02,\n",
      "          3.3678e-02, -6.1754e-02, -2.0802e-02,  2.3411e-02, -1.1431e-03,\n",
      "         -1.9147e-02,  5.2360e-02,  4.9381e-02, -7.7684e-03, -1.3273e-02,\n",
      "         -2.3239e-02, -1.6360e-02, -1.5679e-02,  3.5436e-02,  2.3837e-02,\n",
      "         -4.5328e-02,  4.4684e-02,  5.9803e-02, -5.1521e-02,  5.9575e-02,\n",
      "          6.0208e-02,  4.1223e-02,  3.4474e-02,  3.3652e-02,  1.9058e-02,\n",
      "         -3.1806e-02, -9.2771e-03,  8.5491e-04,  4.6512e-02,  3.0988e-02,\n",
      "          1.4458e-02,  3.7974e-02,  6.7634e-03, -2.7842e-02, -5.0820e-02,\n",
      "          3.1391e-02,  5.4846e-02,  2.8335e-02, -3.7431e-02, -3.2303e-02,\n",
      "         -5.4199e-03, -7.6448e-03,  5.6273e-02,  7.0092e-03, -5.7896e-02,\n",
      "          3.7267e-02,  5.3918e-02, -5.5240e-02,  5.6789e-02, -4.1530e-02,\n",
      "         -1.1353e-02,  5.9292e-02, -2.0899e-02,  3.6858e-02, -2.1314e-02,\n",
      "          6.1733e-02,  5.2413e-02, -5.3848e-02,  4.7119e-02, -4.6036e-02,\n",
      "         -5.0774e-02, -5.8555e-02, -7.9800e-03, -6.2273e-02,  1.2404e-02,\n",
      "         -4.8880e-02, -1.1934e-02,  5.9657e-03,  3.5951e-02,  3.2116e-02,\n",
      "          4.0892e-02, -7.3041e-03,  4.6372e-02, -3.6065e-02,  3.6950e-02,\n",
      "          1.7382e-02,  1.9343e-02, -6.9116e-03,  7.1731e-03,  4.9360e-02,\n",
      "          5.6040e-03,  3.7321e-02, -6.6194e-03,  4.3750e-02,  2.2887e-02,\n",
      "         -4.0433e-02,  2.8588e-02, -2.5543e-02,  3.4765e-02,  3.9451e-02,\n",
      "          1.3296e-02, -1.8275e-02,  4.1352e-02, -3.7913e-02, -3.4808e-02,\n",
      "         -3.6277e-02]], requires_grad=True)\n",
      "linear_b_2\n",
      "Parameter containing:\n",
      "tensor([[ 0.0124, -0.0449,  0.0317,  0.0584,  0.0254, -0.0330,  0.0079, -0.0543,\n",
      "          0.0166,  0.0208,  0.0590, -0.0305, -0.0411, -0.0503,  0.0036, -0.0439,\n",
      "          0.0425, -0.0484,  0.0365,  0.0075, -0.0606,  0.0289, -0.0427,  0.0325,\n",
      "          0.0428,  0.0606, -0.0246, -0.0340, -0.0392, -0.0114, -0.0026,  0.0026,\n",
      "         -0.0058,  0.0376,  0.0543, -0.0463,  0.0464, -0.0620,  0.0163, -0.0132,\n",
      "          0.0606, -0.0436, -0.0102, -0.0185,  0.0217, -0.0433, -0.0528, -0.0303,\n",
      "          0.0071, -0.0148, -0.0216, -0.0434,  0.0155,  0.0083,  0.0563, -0.0210,\n",
      "         -0.0183, -0.0190,  0.0112, -0.0415,  0.0567, -0.0079,  0.0169, -0.0308,\n",
      "         -0.0329,  0.0237, -0.0069,  0.0362, -0.0094, -0.0236,  0.0018, -0.0298,\n",
      "          0.0301, -0.0333, -0.0267,  0.0347, -0.0325, -0.0594,  0.0344,  0.0544,\n",
      "          0.0203,  0.0544, -0.0110,  0.0099, -0.0393, -0.0290,  0.0087,  0.0067,\n",
      "          0.0519,  0.0343, -0.0430, -0.0533,  0.0553, -0.0098,  0.0380, -0.0415,\n",
      "         -0.0029, -0.0494, -0.0464,  0.0513, -0.0038, -0.0180,  0.0089,  0.0604,\n",
      "         -0.0023, -0.0254,  0.0525, -0.0269, -0.0514,  0.0042, -0.0525,  0.0345,\n",
      "         -0.0047, -0.0122,  0.0084,  0.0292, -0.0575,  0.0530,  0.0108, -0.0304,\n",
      "          0.0565, -0.0437, -0.0617, -0.0242, -0.0433,  0.0446,  0.0439,  0.0126,\n",
      "         -0.0267,  0.0322,  0.0196, -0.0303,  0.0429, -0.0613, -0.0161, -0.0223,\n",
      "          0.0368, -0.0073, -0.0112,  0.0290,  0.0570,  0.0199, -0.0014, -0.0246,\n",
      "         -0.0172,  0.0399,  0.0327, -0.0072,  0.0343, -0.0499, -0.0445, -0.0232,\n",
      "          0.0325,  0.0226,  0.0352,  0.0073,  0.0269, -0.0534,  0.0111, -0.0473,\n",
      "          0.0224, -0.0555, -0.0284,  0.0445,  0.0460, -0.0091, -0.0535,  0.0413,\n",
      "          0.0118,  0.0421,  0.0023, -0.0340,  0.0154,  0.0192, -0.0389,  0.0191,\n",
      "          0.0333, -0.0204,  0.0382, -0.0135,  0.0094,  0.0362,  0.0558, -0.0405,\n",
      "          0.0262,  0.0203,  0.0489, -0.0230,  0.0371, -0.0184,  0.0308,  0.0428,\n",
      "          0.0244, -0.0477, -0.0503, -0.0516,  0.0427, -0.0558,  0.0276, -0.0164,\n",
      "          0.0380,  0.0313, -0.0386,  0.0073,  0.0055, -0.0379,  0.0330, -0.0031,\n",
      "          0.0078, -0.0229,  0.0392, -0.0593,  0.0412,  0.0121, -0.0508, -0.0052,\n",
      "         -0.0594,  0.0118, -0.0289,  0.0472,  0.0333, -0.0107, -0.0603,  0.0433,\n",
      "          0.0490, -0.0406, -0.0324, -0.0399, -0.0345, -0.0474, -0.0298, -0.0595,\n",
      "         -0.0396, -0.0053, -0.0229, -0.0438,  0.0336, -0.0500, -0.0136, -0.0035,\n",
      "         -0.0233, -0.0463,  0.0200,  0.0284,  0.0398,  0.0455, -0.0109, -0.0504,\n",
      "          0.0572, -0.0189,  0.0075,  0.0497, -0.0135, -0.0041,  0.0584, -0.0282]],\n",
      "       requires_grad=True)\n",
      "linear_b_3\n",
      "Parameter containing:\n",
      "tensor([[-0.0048]], requires_grad=True)\n",
      "linear_w_0\n",
      "Parameter containing:\n",
      "tensor([[ 0.1010, -0.1090,  0.0487,  ...,  0.1403, -0.1301,  0.1174],\n",
      "        [ 0.1261, -0.0953, -0.0038,  ..., -0.0470,  0.0861, -0.1449],\n",
      "        [ 0.0480,  0.0404, -0.0509,  ..., -0.1144,  0.1320,  0.0621],\n",
      "        ...,\n",
      "        [-0.1476,  0.0032,  0.0181,  ..., -0.1518,  0.0738,  0.0613],\n",
      "        [-0.1200,  0.1401, -0.1275,  ..., -0.0030, -0.0010, -0.0174],\n",
      "        [ 0.0351,  0.0828,  0.0311,  ...,  0.0239, -0.0836, -0.1281]],\n",
      "       requires_grad=True)\n",
      "linear_w_1\n",
      "Parameter containing:\n",
      "tensor([[ 3.1899e-02, -2.8000e-03, -1.2444e-02,  ...,  5.8314e-02,\n",
      "         -4.5148e-02,  2.3891e-02],\n",
      "        [ 1.0775e-02, -1.0873e-02,  5.5141e-02,  ...,  5.3123e-06,\n",
      "         -5.5086e-03,  4.1056e-02],\n",
      "        [-2.5026e-02,  3.0209e-02,  1.7427e-02,  ...,  1.2902e-02,\n",
      "          4.4842e-02, -5.5049e-02],\n",
      "        ...,\n",
      "        [ 2.9715e-02, -1.6260e-02, -3.1139e-02,  ...,  5.1073e-02,\n",
      "          4.7664e-02, -3.3253e-03],\n",
      "        [ 3.1222e-02,  4.9224e-02,  5.4959e-02,  ...,  3.4387e-02,\n",
      "          3.7439e-02,  5.7562e-02],\n",
      "        [-6.1965e-02,  4.4548e-03, -1.2466e-02,  ...,  4.6571e-02,\n",
      "          4.2756e-03, -2.1771e-02]], requires_grad=True)\n",
      "linear_w_2\n",
      "Parameter containing:\n",
      "tensor([[-0.0610, -0.0403, -0.0421,  ..., -0.0556, -0.0272, -0.0543],\n",
      "        [ 0.0621,  0.0559, -0.0595,  ...,  0.0543, -0.0057,  0.0216],\n",
      "        [ 0.0211, -0.0486, -0.0606,  ..., -0.0391,  0.0227, -0.0480],\n",
      "        ...,\n",
      "        [ 0.0422,  0.0250, -0.0018,  ...,  0.0200, -0.0624, -0.0619],\n",
      "        [ 0.0524,  0.0102,  0.0011,  ..., -0.0260, -0.0308, -0.0051],\n",
      "        [-0.0572,  0.0512, -0.0046,  ...,  0.0457,  0.0133, -0.0525]],\n",
      "       requires_grad=True)\n",
      "linear_w_3\n",
      "Parameter containing:\n",
      "tensor([[ 0.0197],\n",
      "        [ 0.0355],\n",
      "        [ 0.0462],\n",
      "        [ 0.0062],\n",
      "        [ 0.0561],\n",
      "        [-0.0357],\n",
      "        [ 0.0400],\n",
      "        [-0.0288],\n",
      "        [-0.0410],\n",
      "        [ 0.0265],\n",
      "        [-0.0556],\n",
      "        [-0.0119],\n",
      "        [-0.0012],\n",
      "        [ 0.0293],\n",
      "        [-0.0133],\n",
      "        [ 0.0311],\n",
      "        [-0.0084],\n",
      "        [ 0.0445],\n",
      "        [ 0.0001],\n",
      "        [-0.0514],\n",
      "        [ 0.0006],\n",
      "        [-0.0382],\n",
      "        [ 0.0510],\n",
      "        [-0.0203],\n",
      "        [-0.0403],\n",
      "        [ 0.0274],\n",
      "        [ 0.0570],\n",
      "        [-0.0582],\n",
      "        [-0.0361],\n",
      "        [ 0.0121],\n",
      "        [-0.0319],\n",
      "        [ 0.0365],\n",
      "        [-0.0404],\n",
      "        [ 0.0243],\n",
      "        [ 0.0173],\n",
      "        [ 0.0552],\n",
      "        [-0.0232],\n",
      "        [-0.0194],\n",
      "        [-0.0372],\n",
      "        [-0.0127],\n",
      "        [-0.0337],\n",
      "        [ 0.0270],\n",
      "        [ 0.0484],\n",
      "        [ 0.0302],\n",
      "        [ 0.0422],\n",
      "        [-0.0038],\n",
      "        [-0.0584],\n",
      "        [ 0.0349],\n",
      "        [-0.0552],\n",
      "        [ 0.0334],\n",
      "        [-0.0233],\n",
      "        [-0.0384],\n",
      "        [-0.0426],\n",
      "        [-0.0355],\n",
      "        [ 0.0462],\n",
      "        [ 0.0474],\n",
      "        [-0.0523],\n",
      "        [-0.0249],\n",
      "        [-0.0080],\n",
      "        [ 0.0320],\n",
      "        [ 0.0355],\n",
      "        [ 0.0107],\n",
      "        [-0.0472],\n",
      "        [ 0.0430],\n",
      "        [ 0.0231],\n",
      "        [ 0.0488],\n",
      "        [-0.0599],\n",
      "        [ 0.0355],\n",
      "        [ 0.0495],\n",
      "        [-0.0541],\n",
      "        [ 0.0513],\n",
      "        [ 0.0324],\n",
      "        [-0.0312],\n",
      "        [ 0.0489],\n",
      "        [ 0.0599],\n",
      "        [-0.0529],\n",
      "        [ 0.0212],\n",
      "        [ 0.0356],\n",
      "        [ 0.0190],\n",
      "        [-0.0482],\n",
      "        [ 0.0227],\n",
      "        [ 0.0547],\n",
      "        [-0.0094],\n",
      "        [-0.0518],\n",
      "        [ 0.0149],\n",
      "        [ 0.0347],\n",
      "        [-0.0090],\n",
      "        [ 0.0611],\n",
      "        [-0.0179],\n",
      "        [ 0.0535],\n",
      "        [-0.0061],\n",
      "        [ 0.0130],\n",
      "        [ 0.0299],\n",
      "        [-0.0335],\n",
      "        [-0.0489],\n",
      "        [-0.0217],\n",
      "        [ 0.0325],\n",
      "        [-0.0496],\n",
      "        [ 0.0226],\n",
      "        [ 0.0194],\n",
      "        [ 0.0412],\n",
      "        [-0.0150],\n",
      "        [ 0.0585],\n",
      "        [-0.0431],\n",
      "        [ 0.0504],\n",
      "        [-0.0585],\n",
      "        [-0.0291],\n",
      "        [ 0.0108],\n",
      "        [ 0.0186],\n",
      "        [-0.0274],\n",
      "        [-0.0597],\n",
      "        [-0.0132],\n",
      "        [ 0.0301],\n",
      "        [ 0.0308],\n",
      "        [-0.0291],\n",
      "        [-0.0146],\n",
      "        [-0.0184],\n",
      "        [-0.0465],\n",
      "        [-0.0241],\n",
      "        [-0.0292],\n",
      "        [ 0.0167],\n",
      "        [ 0.0092],\n",
      "        [-0.0523],\n",
      "        [-0.0102],\n",
      "        [-0.0154],\n",
      "        [ 0.0431],\n",
      "        [-0.0234],\n",
      "        [ 0.0589],\n",
      "        [ 0.0161],\n",
      "        [-0.0402],\n",
      "        [ 0.0559],\n",
      "        [-0.0047],\n",
      "        [-0.0058],\n",
      "        [-0.0277],\n",
      "        [-0.0495],\n",
      "        [-0.0549],\n",
      "        [ 0.0166],\n",
      "        [-0.0241],\n",
      "        [ 0.0341],\n",
      "        [-0.0276],\n",
      "        [-0.0394],\n",
      "        [-0.0093],\n",
      "        [ 0.0067],\n",
      "        [-0.0569],\n",
      "        [-0.0172],\n",
      "        [ 0.0439],\n",
      "        [-0.0006],\n",
      "        [-0.0320],\n",
      "        [-0.0162],\n",
      "        [-0.0351],\n",
      "        [-0.0534],\n",
      "        [-0.0263],\n",
      "        [-0.0064],\n",
      "        [-0.0289],\n",
      "        [ 0.0341],\n",
      "        [-0.0025],\n",
      "        [ 0.0267],\n",
      "        [-0.0424],\n",
      "        [-0.0569],\n",
      "        [ 0.0059],\n",
      "        [-0.0076],\n",
      "        [ 0.0369],\n",
      "        [ 0.0570],\n",
      "        [-0.0359],\n",
      "        [-0.0588],\n",
      "        [-0.0368],\n",
      "        [ 0.0135],\n",
      "        [ 0.0502],\n",
      "        [ 0.0025],\n",
      "        [ 0.0504],\n",
      "        [-0.0165],\n",
      "        [ 0.0168],\n",
      "        [-0.0444],\n",
      "        [-0.0612],\n",
      "        [ 0.0380],\n",
      "        [-0.0598],\n",
      "        [-0.0024],\n",
      "        [-0.0354],\n",
      "        [-0.0436],\n",
      "        [-0.0106],\n",
      "        [ 0.0510],\n",
      "        [ 0.0379],\n",
      "        [ 0.0329],\n",
      "        [-0.0486],\n",
      "        [ 0.0183],\n",
      "        [-0.0520],\n",
      "        [-0.0165],\n",
      "        [ 0.0164],\n",
      "        [-0.0299],\n",
      "        [-0.0514],\n",
      "        [ 0.0139],\n",
      "        [ 0.0448],\n",
      "        [-0.0596],\n",
      "        [-0.0507],\n",
      "        [ 0.0036],\n",
      "        [-0.0017],\n",
      "        [-0.0612],\n",
      "        [-0.0464],\n",
      "        [-0.0210],\n",
      "        [-0.0156],\n",
      "        [-0.0025],\n",
      "        [ 0.0108],\n",
      "        [ 0.0353],\n",
      "        [ 0.0502],\n",
      "        [-0.0537],\n",
      "        [-0.0430],\n",
      "        [-0.0352],\n",
      "        [-0.0378],\n",
      "        [ 0.0069],\n",
      "        [ 0.0210],\n",
      "        [-0.0474],\n",
      "        [-0.0223],\n",
      "        [-0.0457],\n",
      "        [-0.0457],\n",
      "        [ 0.0359],\n",
      "        [-0.0005],\n",
      "        [-0.0577],\n",
      "        [ 0.0197],\n",
      "        [-0.0465],\n",
      "        [ 0.0174],\n",
      "        [ 0.0567],\n",
      "        [-0.0375],\n",
      "        [-0.0355],\n",
      "        [ 0.0555],\n",
      "        [-0.0586],\n",
      "        [ 0.0289],\n",
      "        [ 0.0600],\n",
      "        [-0.0118],\n",
      "        [-0.0525],\n",
      "        [-0.0048],\n",
      "        [ 0.0191],\n",
      "        [-0.0045],\n",
      "        [-0.0130],\n",
      "        [ 0.0566],\n",
      "        [ 0.0139],\n",
      "        [-0.0470],\n",
      "        [ 0.0373],\n",
      "        [-0.0564],\n",
      "        [-0.0356],\n",
      "        [-0.0404],\n",
      "        [ 0.0008],\n",
      "        [-0.0368],\n",
      "        [ 0.0324],\n",
      "        [-0.0186],\n",
      "        [-0.0101],\n",
      "        [ 0.0558],\n",
      "        [ 0.0622],\n",
      "        [-0.0382],\n",
      "        [ 0.0350],\n",
      "        [-0.0440],\n",
      "        [ 0.0617],\n",
      "        [ 0.0519],\n",
      "        [ 0.0545],\n",
      "        [-0.0227],\n",
      "        [-0.0364],\n",
      "        [ 0.0518]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████▌                      | 1/2 [00:04<00:04,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from train: tensor(1029.3239, grad_fn=<MeanBackward0>)\n",
      "linear_b_0\n",
      "Parameter containing:\n",
      "tensor([[ 4.8491e+02, -2.1587e+03, -2.4876e+04,  1.9703e+04, -4.4873e+03,\n",
      "         -1.4558e+04, -8.3768e+03, -1.1541e+04, -1.0110e+04,  1.6738e+03,\n",
      "          3.0163e+04,  3.1570e+04,  3.7682e+03,  1.0785e+04, -2.5457e+04,\n",
      "         -1.0194e+04, -6.5370e+03,  6.0863e+03,  1.4713e+01,  3.6880e+04,\n",
      "          4.1170e+04, -4.2408e+04, -7.1919e+03, -2.0732e+04, -8.5358e+04,\n",
      "          5.4300e+04, -5.3928e+04,  2.1113e+04, -2.9459e+04,  4.8478e+04,\n",
      "         -2.0551e+04,  6.8798e+04,  3.2532e+04, -4.8949e+04, -5.0261e+03,\n",
      "         -8.4922e+04,  3.5083e+04, -3.6453e+04, -8.6894e+02,  2.6285e+04,\n",
      "          6.7956e+04,  9.9828e+03, -4.5017e+03, -3.3751e+04, -6.9078e+04,\n",
      "         -3.3503e+04, -1.9483e+04, -4.2734e+04,  3.2521e+04, -8.4647e+04,\n",
      "          4.3944e+03, -1.4576e+04,  1.4598e+04,  5.0616e+03,  6.4704e+03,\n",
      "          2.4878e+03,  1.4584e+04, -2.8065e+03,  2.0062e+04, -4.0945e+04,\n",
      "          4.6109e+03,  1.2317e+04,  1.1517e+05,  5.6341e+03, -8.8019e+02,\n",
      "          3.3254e+04,  4.2932e+04, -8.1191e+03, -2.0536e+04, -1.2113e+04,\n",
      "          7.2134e+03, -3.5794e+04, -4.5320e+04, -1.9181e+04, -6.1699e+03,\n",
      "          3.0230e+04, -1.0193e+04, -4.3845e+04, -4.9698e+04,  1.6870e+04,\n",
      "         -6.1127e+04,  8.4975e+04, -5.2903e+03,  3.1781e+04,  4.8758e+04,\n",
      "         -6.6510e+03, -4.8667e+04, -1.1086e+04,  3.8149e+04,  1.9986e+04,\n",
      "          1.4827e+04,  3.1333e+04, -1.6961e+04, -6.3731e+04, -3.7278e+04,\n",
      "         -3.4594e+04,  3.6546e+04,  1.5503e+04,  4.3816e+04, -4.4273e+03,\n",
      "         -2.7407e+04, -3.8167e+03,  2.7095e+04,  1.2194e+04, -3.8863e+04,\n",
      "          4.6009e+04, -1.4356e+04, -1.8800e+04, -5.4719e+02,  6.2127e+02,\n",
      "         -1.0614e+04, -7.7681e+04,  2.7056e+04,  1.8764e+04,  3.8506e+04,\n",
      "         -9.4675e+04,  2.3878e+03, -1.4372e+04, -5.0033e+04, -3.2253e+04,\n",
      "          9.0364e+02, -7.2987e+04,  9.6673e+03,  2.3352e+04,  1.6810e+04,\n",
      "          3.9231e+04,  5.6575e+04, -3.0373e+04,  2.2777e+04, -1.1016e+04,\n",
      "          3.0752e+04,  4.5742e+04,  3.2255e+04,  1.2563e+04,  1.0741e+04,\n",
      "         -4.4919e+03, -8.1904e+03, -5.4002e+04,  1.5435e+04,  1.6688e+03,\n",
      "          7.0651e+03,  7.5425e+03, -2.3944e+04, -1.4595e+04, -4.5395e+03,\n",
      "          6.7601e+03,  2.2494e+04,  1.1649e+04,  2.4049e+04, -7.2819e+02,\n",
      "         -7.8062e+04,  4.3968e+03, -1.6405e+04, -5.6998e+04, -6.2813e+04,\n",
      "         -9.0277e+04,  1.7991e+04,  2.6366e+03, -3.7485e+04, -3.7545e+04,\n",
      "          4.3322e+04,  2.8303e+03,  3.6718e+04,  2.4379e+04, -4.2063e+04,\n",
      "         -5.2314e+04,  5.9440e+03, -6.3121e+03, -1.3952e+04,  4.5212e+04,\n",
      "          9.7642e+03,  2.6487e+02, -2.7573e+04, -1.1568e+03, -7.7166e+04,\n",
      "          3.1352e+03, -3.7397e+04,  2.5528e+03, -6.3697e+03, -2.9318e+02,\n",
      "         -2.3356e+04,  7.1528e+03,  3.6781e+04, -2.0755e+04, -1.7135e+04,\n",
      "         -4.4584e+04,  3.0376e+03, -6.1882e+04,  2.0378e+04, -1.4083e+04,\n",
      "          5.9123e+04, -3.6481e+04, -1.8370e+04, -9.3282e+03, -9.2555e+02,\n",
      "         -1.3970e+04, -4.5545e+04, -5.0877e+03, -4.6922e+03, -2.8553e+04,\n",
      "         -2.5967e+03,  1.7831e+03,  1.9235e+03, -3.8573e+04, -1.4019e+03,\n",
      "          2.7266e+03, -6.1923e+04, -4.7112e+04,  1.3904e+04,  3.2713e+03,\n",
      "          2.0043e+03,  1.4891e+04,  5.4351e+04,  6.0235e+04,  4.4300e+04,\n",
      "          7.2908e+04,  2.6864e+04,  9.7437e+04,  1.5287e+03, -1.9347e+04,\n",
      "          1.6118e+04, -8.6343e+03,  2.6246e+04, -1.5444e+04, -1.3290e+04,\n",
      "         -9.2842e+01,  6.1658e+04,  3.1401e+03,  2.0481e+04,  2.7841e+04,\n",
      "         -1.2812e+04,  1.3112e+04,  5.3721e+02, -5.0030e+04, -2.2531e+04,\n",
      "         -6.9895e+03, -2.1461e+04,  6.9889e+04,  4.5889e+04,  3.8631e+04,\n",
      "         -1.1295e+03, -2.9046e+04,  1.4287e+03,  1.1353e+04, -1.0047e+04,\n",
      "          2.3296e+04, -6.4137e+02, -3.0543e+04,  4.7862e+02,  5.3785e+03,\n",
      "         -7.9964e+03, -4.7802e+04, -3.6105e+04, -2.4503e+04,  2.9999e+04,\n",
      "         -2.4236e+04]], requires_grad=True)\n",
      "linear_b_1\n",
      "Parameter containing:\n",
      "tensor([[-8.2116e+04, -6.8977e+04,  4.0909e+04, -1.6012e+04,  1.2657e+05,\n",
      "         -2.2311e+03,  2.6582e+03, -3.9423e+03, -1.2042e+05,  1.7376e+04,\n",
      "          1.9455e+04,  9.5410e+04,  6.0631e+04, -6.4531e+04,  3.3341e+03,\n",
      "          1.7842e+05,  9.2385e+03,  2.3166e+04, -1.1182e+05,  8.2131e+04,\n",
      "         -1.4530e+04,  3.9170e+04, -9.8675e+04, -1.6620e+04, -5.4670e+03,\n",
      "         -1.1990e+05,  3.0598e+04, -4.7553e+02,  3.9330e+04, -8.5255e+04,\n",
      "         -6.1141e+04,  2.3226e+03,  3.3796e+04, -3.0080e+04,  3.0261e+02,\n",
      "          2.2516e+03,  2.0601e+05, -4.1866e+04,  7.9821e+04,  2.4693e+04,\n",
      "          6.7227e+04,  2.0631e+04,  7.6354e+04,  6.2447e+04,  8.9491e+04,\n",
      "         -3.4070e+02,  2.3274e+02, -6.9274e+04, -9.1716e+04, -5.3929e+04,\n",
      "         -5.8563e+04,  9.5398e+03,  4.0710e+04, -9.1366e+03,  4.4903e+04,\n",
      "         -5.2937e+04, -3.4645e+04, -9.1036e+04, -1.6152e+05, -2.0360e+04,\n",
      "         -9.9125e+04,  7.0956e+04, -1.3624e+04, -6.6664e+04,  9.8987e+02,\n",
      "          4.9799e+04,  2.3244e+04, -1.3333e+05, -9.7455e+04, -4.9221e+04,\n",
      "          1.4408e+04,  1.7777e+05, -5.6379e+04,  2.5659e+04, -2.6089e+05,\n",
      "         -1.0092e+05,  1.5192e+04, -3.6936e+03, -1.4161e+05, -5.0180e+04,\n",
      "         -5.6436e+04,  1.6018e+04,  1.7024e+03, -5.0210e+03,  7.2199e+04,\n",
      "          6.2725e+04,  1.2662e+04,  9.2655e+04, -1.5401e+05, -1.6880e+05,\n",
      "          7.5155e+04, -1.6596e+04,  9.5953e+04,  6.6582e+04,  3.2169e+04,\n",
      "         -1.2366e+04, -2.4350e+04, -7.1633e+04, -7.3704e+04, -1.1045e+03,\n",
      "          7.7931e+04, -1.2955e+03,  1.6409e+05,  2.8452e+03, -2.7798e+03,\n",
      "          1.4055e+04, -6.9405e+03,  1.4132e+05, -6.5937e+03,  2.7500e+04,\n",
      "          3.6808e+04,  2.6474e+04,  3.1306e+04, -6.1301e+02, -4.2495e+04,\n",
      "         -2.2407e+04, -2.3293e+04, -1.4462e+05,  1.9242e+05, -3.6057e+03,\n",
      "         -5.6615e+04,  2.1041e+04, -4.7926e+04, -7.3355e+03,  2.2167e+05,\n",
      "         -1.8347e+03,  1.2247e+05, -5.6518e+04, -1.4864e+05,  6.9980e+02,\n",
      "          5.6714e+04, -5.3838e+04, -2.4850e+04,  2.2791e+04,  2.1699e+04,\n",
      "          1.4823e+04,  9.8849e+04, -1.8672e+04, -3.8952e+04, -1.1872e+03,\n",
      "         -9.1302e+04,  4.0748e+04,  3.4961e+03, -3.7330e+04, -1.2228e+05,\n",
      "         -2.0627e+04,  1.6530e+04, -3.4645e+04,  1.3611e+05, -3.6066e+03,\n",
      "         -8.9382e+04,  1.7992e+03,  1.8337e+05, -1.5750e+03,  2.3869e+04,\n",
      "         -2.1275e+04,  3.3960e+04, -9.3620e+03,  5.9469e+04,  5.4647e+04,\n",
      "         -7.4262e+04,  5.7239e+04, -3.4350e+03, -3.9044e+04, -4.3071e+04,\n",
      "          2.3431e+04,  3.9252e+04,  6.2407e+03,  3.5279e+04,  5.3009e+04,\n",
      "         -5.4310e+04,  1.3901e+04,  3.3032e+04, -3.1772e+04,  2.5574e+03,\n",
      "         -5.0562e+04,  3.0964e+05, -1.1762e+04,  1.8448e+04,  6.2462e+04,\n",
      "          6.3505e+03,  3.4807e+04,  8.0526e+04, -2.0389e+05,  6.0111e+03,\n",
      "          2.6640e+03, -8.7567e+04, -1.3511e+04, -8.7538e+04,  3.5823e+04,\n",
      "         -4.2857e+04, -4.6529e+04,  4.6234e+04,  1.9626e+05, -5.0462e+03,\n",
      "          9.3678e+04, -6.6536e+04,  1.7708e+02,  9.2993e+04,  1.0815e+05,\n",
      "          6.6137e+04, -6.4762e+03, -2.6881e+04, -1.4540e+05,  2.1748e+05,\n",
      "          3.6640e+03,  7.3574e+03,  2.5003e+04,  5.3743e+04,  2.0767e+04,\n",
      "          1.5041e+04,  9.8853e+04, -2.6315e+03,  1.6309e+05, -7.9784e+02,\n",
      "          8.4152e+04, -2.2765e+05,  9.5758e+04,  9.6182e+04, -2.7420e+04,\n",
      "         -5.9009e+04,  6.6294e+04, -1.9590e+04, -5.2251e+04, -6.5987e+04,\n",
      "         -6.9876e+03, -5.0248e+02, -5.1054e+04, -4.4196e+04,  8.0774e+04,\n",
      "          3.5759e+04, -1.1131e+05, -8.1065e+04,  1.0495e+03, -3.1300e+04,\n",
      "         -3.3934e+04, -1.5456e+04, -1.0517e+05,  8.5591e+04, -1.2975e+05,\n",
      "          4.9215e+04,  6.7677e+04,  9.0567e+04,  1.3121e+05,  4.4535e+03,\n",
      "          2.5238e+03,  1.8414e+04, -7.6198e+04, -1.3439e+03, -5.0566e+04,\n",
      "          8.4624e+04, -7.7428e+03, -4.7028e+04,  2.7862e+04, -9.2263e+03,\n",
      "          7.7741e+02]], requires_grad=True)\n",
      "linear_b_2\n",
      "Parameter containing:\n",
      "tensor([[-4.0488e+05, -7.4581e+04,  2.2999e+04, -4.2998e+04,  2.1013e+05,\n",
      "          7.7439e+01, -5.1858e+04, -5.5014e+04,  1.1169e+05, -8.0326e+04,\n",
      "          1.4741e+05, -3.4906e+04, -1.4970e+03,  1.3807e+05, -4.3151e+04,\n",
      "          6.2722e+02,  9.1549e+03,  3.5587e+05,  1.2245e+04, -1.6949e+05,\n",
      "          2.3038e+02, -1.0740e+05,  5.0497e+05,  1.5521e+05,  1.4724e+05,\n",
      "         -1.3413e+05, -1.1440e+05, -4.8610e+04, -1.6224e+05, -3.3063e+04,\n",
      "         -7.4046e+02, -8.9674e+04,  7.7411e+01,  7.9284e+03, -6.7937e+03,\n",
      "          2.4250e+05,  1.2717e+05, -8.1350e+04, -9.8195e+04, -2.9064e+04,\n",
      "          5.5803e+03,  4.2202e+04, -1.3015e+05,  1.8455e+05, -4.1124e+05,\n",
      "         -1.5652e+02, -1.1024e+05, -1.8616e+04, -1.0821e+05,  6.1485e+04,\n",
      "         -1.6816e+05, -1.4788e+05,  1.3528e+05,  1.1537e+05, -1.4179e+05,\n",
      "          9.8613e+04, -2.0517e+05, -2.1802e+05,  3.6092e+04,  7.8229e+01,\n",
      "          2.1339e+03,  7.2204e+04,  4.6236e+05, -1.0531e+05, -7.4075e+02,\n",
      "          2.0807e+04,  1.7130e+05, -1.5844e+05, -9.3910e+04,  7.8278e+04,\n",
      "          3.8016e+02,  1.9076e+05,  5.5637e+04,  9.9547e+04,  6.6955e+01,\n",
      "          2.7517e+05,  1.3824e+05,  1.0671e+05,  1.1071e+04,  3.2514e+04,\n",
      "         -2.5126e+04,  3.4058e+03,  3.1513e+04,  1.5452e+05,  6.6862e+04,\n",
      "          1.9048e+05,  2.4923e+04,  1.6508e+03,  3.7555e+04, -4.1058e+04,\n",
      "         -9.1368e+03,  4.2476e+03, -1.0931e+05, -2.0444e+05,  4.1141e+03,\n",
      "         -2.7753e+03,  8.2767e+03, -3.8677e+05,  4.6142e+04,  1.2785e+04,\n",
      "          1.1845e+05, -2.2969e+04, -3.2382e+05,  2.3696e+04, -1.6674e+04,\n",
      "          3.3406e+03,  5.1857e+04,  3.3188e+04, -9.1450e+01,  6.2759e+04,\n",
      "         -2.0196e+05,  9.9318e+03, -1.0957e+05,  7.8734e+03, -1.1406e+04,\n",
      "          8.1000e+03, -5.0857e+04,  1.6923e+05,  2.3862e+03, -4.6411e+04,\n",
      "         -6.7187e+04, -1.5780e+04, -2.6452e+03, -5.5121e+03, -2.5064e+04,\n",
      "         -1.8191e+05,  1.1326e+05, -4.2466e+05,  7.8057e+01, -7.8627e+04,\n",
      "         -4.1510e+05, -1.1593e+04,  2.1148e+04, -1.2624e+05, -3.8961e+05,\n",
      "         -2.3100e+05, -7.7566e+04,  2.4734e+04,  1.0972e+05,  1.3986e+04,\n",
      "          4.7906e+03,  2.0749e+04,  1.1995e+03,  3.6614e+04,  2.2585e+04,\n",
      "         -2.5832e+05,  7.4819e+03, -9.5222e+04,  1.9612e+04, -7.1907e+04,\n",
      "          9.7296e+03, -8.6198e+04,  3.4657e+04,  2.1392e+04,  5.2232e+03,\n",
      "          1.1739e+04, -1.2159e+04, -3.9856e+04, -6.1154e+04,  7.1242e+01,\n",
      "          8.6412e+03,  1.1600e+05,  1.8734e+03,  6.4491e+03, -7.8185e+02,\n",
      "          1.5862e+04, -5.8110e+03, -9.7041e+04,  1.1103e+03, -5.0101e+05,\n",
      "         -1.3471e+04,  3.3886e+04,  1.7348e+05, -3.1308e+04,  3.1092e+05,\n",
      "          3.1272e+05,  7.7865e+03,  7.0026e+03,  1.6335e+04,  1.6389e+04,\n",
      "         -1.9755e+05,  6.5571e+03,  4.0664e+03,  1.1338e+04, -1.0796e+05,\n",
      "         -2.1083e+05,  8.2790e+04,  8.2963e+04,  4.5259e+03, -1.8133e+04,\n",
      "          1.0844e+04, -2.0697e+05,  6.2428e+04, -7.5241e+04,  2.2094e+04,\n",
      "         -5.9294e+02,  5.6127e+03,  5.5086e+02,  6.7985e+04, -4.7063e+04,\n",
      "          1.6029e+04, -2.5714e+04,  4.2586e+03, -1.4780e+04,  1.3192e+05,\n",
      "         -7.7476e+04,  4.9723e+04, -2.9117e+04, -1.0658e+03,  6.7229e+04,\n",
      "          2.8010e+05, -2.1737e+05,  1.3392e+05, -3.8504e+04, -5.5561e+03,\n",
      "         -2.4004e+03,  1.9733e+04,  7.1792e+04, -2.0019e+05, -9.5528e+02,\n",
      "         -1.9739e+05,  3.6057e+04, -9.1801e+01, -5.5453e+04,  1.9672e+04,\n",
      "          1.3529e+04, -3.3034e+05, -9.7741e+04, -1.5819e+05,  7.7789e+01,\n",
      "         -1.0487e+04, -1.6571e+03, -5.4269e+04, -1.3233e+05,  7.8035e+01,\n",
      "          7.7343e+01, -3.0985e+05, -4.2116e+04, -1.1328e+05,  4.3588e+03,\n",
      "         -1.9797e+02, -2.0652e+05,  1.2063e+05,  9.4679e+03,  7.7775e+03,\n",
      "          1.0371e+03, -5.2690e+04, -3.2659e+05, -1.8869e+04,  2.2665e+04,\n",
      "          2.4130e+05,  2.4573e+03, -8.8183e+04,  2.6084e+04,  1.0674e+05,\n",
      "          3.6044e+04]], requires_grad=True)\n",
      "linear_b_3\n",
      "Parameter containing:\n",
      "tensor([[1180.6515]], requires_grad=True)\n",
      "linear_w_0\n",
      "Parameter containing:\n",
      "tensor([[ 4505.0679,  -744.8229, -3625.1428,  ..., -2472.6924,  5213.7959,\n",
      "         -2279.1067],\n",
      "        [ 4505.0928,  -744.8093, -3625.1953,  ..., -2472.8796,  5214.0117,\n",
      "         -2279.3691],\n",
      "        [ 4505.0146,  -744.6735, -3625.2422,  ..., -2472.9473,  5214.0576,\n",
      "         -2279.1619],\n",
      "        ...,\n",
      "        [ 4504.8193,  -744.7108, -3625.1733,  ..., -2472.9846,  5213.9995,\n",
      "         -2279.1628],\n",
      "        [ 4504.8467,  -744.5739, -3625.3188,  ..., -2472.8357,  5213.9248,\n",
      "         -2279.2415],\n",
      "        [ 4505.0020,  -744.6312, -3625.1604,  ..., -2472.8088,  5213.8423,\n",
      "         -2279.3523]], requires_grad=True)\n",
      "linear_w_1\n",
      "Parameter containing:\n",
      "tensor([[15390.2393, 28072.1152, 20343.3320,  ..., -4622.4492, 40067.1875,\n",
      "          -510.9043],\n",
      "        [15390.2178, 28072.1055, 20343.3984,  ..., -4622.5073, 40067.2305,\n",
      "          -510.8871],\n",
      "        [15390.1816, 28072.1465, 20343.3613,  ..., -4622.4946, 40067.2773,\n",
      "          -510.9832],\n",
      "        ...,\n",
      "        [15390.2363, 28072.1016, 20343.3125,  ..., -4622.4561, 40067.2812,\n",
      "          -510.9315],\n",
      "        [15390.2383, 28072.1660, 20343.3984,  ..., -4622.4731, 40067.2734,\n",
      "          -510.8706],\n",
      "        [15390.1455, 28072.1211, 20343.3320,  ..., -4622.4609, 40067.2383,\n",
      "          -510.9499]], requires_grad=True)\n",
      "linear_w_2\n",
      "Parameter containing:\n",
      "tensor([[ 41916.8516,  -1070.5872,  -5099.3091,  ..., -13157.1904,\n",
      "         -16417.6602,  34894.7305],\n",
      "        [ 41916.9766,  -1070.4910,  -5099.3267,  ..., -13157.0801,\n",
      "         -16417.6387,  34894.8086],\n",
      "        [ 41916.9336,  -1070.5955,  -5099.3276,  ..., -13157.1738,\n",
      "         -16417.6094,  34894.7383],\n",
      "        ...,\n",
      "        [ 41916.9570,  -1070.5220,  -5099.2690,  ..., -13157.1143,\n",
      "         -16417.6953,  34894.7227],\n",
      "        [ 41916.9648,  -1070.5367,  -5099.2661,  ..., -13157.1611,\n",
      "         -16417.6641,  34894.7812],\n",
      "        [ 41916.8555,  -1070.4956,  -5099.2715,  ..., -13157.0889,\n",
      "         -16417.6191,  34894.7344]], requires_grad=True)\n",
      "linear_w_3\n",
      "Parameter containing:\n",
      "tensor([[2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500],\n",
      "        [2007979.2500]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [00:09<00:00,  4.95s/it]\n"
     ]
    }
   ],
   "source": [
    "model.train(X, y, task_lengths, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame()\n",
    "for name in os.listdir(\"../datasets/mw_valid\"):\n",
    "    if not (name.startswith('.')):\n",
    "        dir_name = 'mw_valid/'+name\n",
    "        print(dir_name)\n",
    "        df = read_file(dir_name)\n",
    "        test = pd.concat([data, df])\n",
    "\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "Test = PreferenceMAML(test, input_size, hidden_size1, hidden_size2, output_size)\n",
    "test_X, test_y = Test.prepare_data(k=4)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, X, y):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X)):\n",
    "            X_tensor = torch.tensor(X[i], dtype=torch.float32)\n",
    "            output = model.model(X_tensor.unsqueeze(0))  \n",
    "            predictions.append(output.squeeze().numpy())  \n",
    "\n",
    "    preds = []\n",
    "    for _ in range(len(predictions)):\n",
    "        preds.append((np.array(predictions[_]).mean()))\n",
    "\n",
    "    pred_label = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_label.append([0] if preds[i]>0.5 else [1])\n",
    "    \n",
    "    sum = 0\n",
    "    for _ in range(len(y)):\n",
    "        sum += pred_label[_]==y[_]\n",
    "    accuracy = sum/len(y)\n",
    "    return accuracy, pred_label\n",
    "\n",
    "test_accuracy, pred_labels = evaluate_model(model, test_X, test_y)\n",
    "print(f'\\nTest Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without INNER LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import pandas as pd\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "#         self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "#         self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.sigmoid(self.fc3(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class PreferenceMAML:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         ml10,\n",
    "#         input_size,\n",
    "#         hidden_size1,\n",
    "#         hidden_size2,\n",
    "#         output_size,\n",
    "#         num_support=10,\n",
    "#         num_query=10,\n",
    "#         num_inner_steps=5,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         self.ml10 = ml10\n",
    "#         self.reward_criterion =  nn.CrossEntropyLoss()\n",
    "#         self.num_support = num_support\n",
    "#         self.num_query = num_query\n",
    "#         self.num_inner_steps = num_inner_steps\n",
    "\n",
    "#         self.model = Model(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "#     def construct_episodes(self):\n",
    "#         episodes = []\n",
    "#         episode = []\n",
    "#         for _, row in self.ml10.iterrows():\n",
    "#             episode.append(row)\n",
    "#             if row['done']:\n",
    "#                 episodes.append(episode)\n",
    "#                 episode = []\n",
    "#         return episodes\n",
    "\n",
    "\n",
    "\n",
    "#     def form_sigma_groups(self, episode, k):\n",
    "#         sigmas = []\n",
    "#         segments = []\n",
    "#         q, r = divmod(len(episode), k)\n",
    "#         for i in range(k):\n",
    "#             segments.append(episode[i*q+min(i,r) : (i+1)*q+min(i+1,r)])\n",
    "\n",
    "#         for i in range(k):\n",
    "#             sigma_i = segments[i]\n",
    "#             for j in range(i+1, k):\n",
    "#                 sigma_j = segments[j]\n",
    "\n",
    "#                 sigmas.append((sigma_i, sigma_j))\n",
    "#         return sigmas\n",
    "\n",
    "#     def compare_probabilities(self, sigma1, sigma2):\n",
    "#         exp_sum_rewards_sigma1 = np.exp(sum(row['reward'] for row in sigma1))\n",
    "#         exp_sum_rewards_sigma2 = np.exp(sum(row['reward'] for row in sigma2))\n",
    "#         prob = exp_sum_rewards_sigma1 / (exp_sum_rewards_sigma1 + exp_sum_rewards_sigma2)\n",
    "#         return [1,0] if prob > 0.5 else [0,1]\n",
    "\n",
    "\n",
    "#     def prepare_data(self, k):\n",
    "#         X = []\n",
    "#         y = []\n",
    "#         episodes = self.construct_episodes()\n",
    "#         for episode in episodes:\n",
    "#             sigmas = self.form_sigma_groups(episode, k)\n",
    "#             for _ in range(len(sigmas)):\n",
    "\n",
    "#                 sigma1 = sigmas[_][0]\n",
    "#                 sigma2 = sigmas[_][1]\n",
    "\n",
    "#                 obs_action_sigma1 = []\n",
    "#                 for row in sigma1:\n",
    "#                     obs_action = list(row['obs']) + list(row['action'])  # Concatenate obs and action\n",
    "#                     obs_action_sigma1.append(obs_action)\n",
    "\n",
    "#                 obs_action_sigma2 = []\n",
    "#                 for row in sigma2:\n",
    "#                     obs_action = list(row['obs']) + list(row['action'])  # Concatenate obs and action\n",
    "#                     obs_action_sigma2.append(obs_action)\n",
    "\n",
    "#                 if len(obs_action_sigma1) > len(obs_action_sigma2):\n",
    "#                     obs_action_sigma1 = obs_action_sigma1[1:]\n",
    "#                 elif len(obs_action_sigma1) < len(obs_action_sigma2):\n",
    "#                     obs_action_sigma2 = obs_action_sigma2[1:]\n",
    "#                 else:\n",
    "#                     continue\n",
    "\n",
    "#                 X.append(np.concatenate((obs_action_sigma1, obs_action_sigma2), axis = 1))\n",
    "#                 y.append([self.compare_probabilities(sigma1, sigma2)]) \n",
    "\n",
    "#         return X, y\n",
    "\n",
    "\n",
    "#     def setup_optimizers(self, optim_class, optim_kwargs):\n",
    "#         self.optim = optim_class(self.model.parameters(), **optim_kwargs)\n",
    "\n",
    "#     def _train_step(self, X, y):\n",
    "#         self.optim.zero_grad()\n",
    "#         loss = self._outer_step(X, y)\n",
    "#         loss.backward()\n",
    "#         self.optim.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     def _outer_step(self, X, y):\n",
    "#         outer_losses = []\n",
    "#         for i in range(len(X)):\n",
    "#             loss = self._compute_loss(X[i], y[i])\n",
    "#             outer_losses.append(loss)\n",
    "#         return torch.mean(torch.stack(outer_losses))\n",
    "\n",
    "#     def _compute_loss(self, X, y):\n",
    "#         X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "#         y_tensor = torch.tensor([y], dtype=torch.float32)\n",
    "#         output = self.model(X_tensor)\n",
    "#         output_flat = output.view(-1)\n",
    "#         y_flat = y_tensor.view(-1)\n",
    "#         loss = self.reward_criterion(output_flat[-2:], y_flat)\n",
    "#         return loss\n",
    "\n",
    "# ml10 = data.copy()  \n",
    "# input_size = 86  # Assuming obs has 39 numbers and action has 4 numbers * 2 for pair of sigmas\n",
    "# hidden_size1 = 128\n",
    "# hidden_size2 = 128\n",
    "# output_size = 2\n",
    "# num_epochs = 20\n",
    "\n",
    "# model = PreferenceMAML(ml10, input_size, hidden_size1, hidden_size2, output_size)\n",
    "# model.setup_optimizers(optim.Adam, {\"lr\": 0.005})\n",
    "\n",
    "# X, y = model.prepare_data(k=4)\n",
    "\n",
    "# # Train the model\n",
    "# for epoch in range(num_epochs):\n",
    "#     loss = model._train_step(X, y)\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With INNER LOOP but Improper classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "#         self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "#         self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.sigmoid(self.fc3(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class PreferenceMAML:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         ml10,\n",
    "#         input_size,\n",
    "#         hidden_size1,\n",
    "#         hidden_size2,\n",
    "#         output_size,\n",
    "#         inner_lr = 0.01,\n",
    "#         num_support=10,\n",
    "#         num_query=10,\n",
    "#         num_inner_steps=5,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         self.ml10 = ml10\n",
    "#         self.reward_criterion =  nn.CrossEntropyLoss()\n",
    "#         self.num_support = num_support\n",
    "#         self.num_query = num_query\n",
    "#         self.num_inner_steps = num_inner_steps\n",
    "#         self.inner_lr = inner_lr\n",
    "\n",
    "#         self.model = Model(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "#     def construct_episodes(self):\n",
    "#         episodes = []\n",
    "#         episode = []\n",
    "#         for _, row in self.ml10.iterrows():\n",
    "#             episode.append(row)\n",
    "#             if row['done']:\n",
    "#                 episodes.append(episode)\n",
    "#                 episode = []\n",
    "#         return episodes\n",
    "\n",
    "#     def form_sigma_groups(self, episode, k):\n",
    "#         sigmas = []\n",
    "#         segments = []\n",
    "#         q, r = divmod(len(episode), k)\n",
    "#         for i in range(k):\n",
    "#             segments.append(episode[i*q+min(i,r) : (i+1)*q+min(i+1,r)])\n",
    "\n",
    "#         for i in range(k):\n",
    "#             sigma_i = segments[i]\n",
    "#             for j in range(i+1, k):\n",
    "#                 sigma_j = segments[j]\n",
    "\n",
    "#                 sigmas.append((sigma_i, sigma_j))\n",
    "#         return sigmas\n",
    "\n",
    "#     def compare_probabilities(self, sigma1, sigma2):\n",
    "#         exp_sum_rewards_sigma1 = np.exp(sum(row['reward'] for row in sigma1))\n",
    "#         exp_sum_rewards_sigma2 = np.exp(sum(row['reward'] for row in sigma2))\n",
    "#         prob = exp_sum_rewards_sigma1 / (exp_sum_rewards_sigma1 + exp_sum_rewards_sigma2)\n",
    "#         return [1,0] if prob > 0.5 else [0,1]\n",
    "\n",
    "#     def prepare_data(self, k):\n",
    "#         X = []\n",
    "#         y = []\n",
    "#         episodes = self.construct_episodes()\n",
    "#         for episode in episodes:\n",
    "#             sigmas = self.form_sigma_groups(episode, k)\n",
    "#             for _ in range(len(sigmas)):\n",
    "#                 sigma1 = sigmas[_][0]\n",
    "#                 sigma2 = sigmas[_][1]\n",
    "\n",
    "#                 obs_action_sigma1 = []\n",
    "#                 for row in sigma1:\n",
    "#                     obs_action = list(row['obs']) + list(row['action'])  # Concatenate obs and action\n",
    "#                     obs_action_sigma1.append(obs_action)\n",
    "\n",
    "#                 obs_action_sigma2 = []\n",
    "#                 for row in sigma2:\n",
    "#                     obs_action = list(row['obs']) + list(row['action'])  # Concatenate obs and action\n",
    "#                     obs_action_sigma2.append(obs_action)\n",
    "\n",
    "#                 if len(obs_action_sigma1) > len(obs_action_sigma2):\n",
    "#                     obs_action_sigma1 = obs_action_sigma1[1:]\n",
    "#                 elif len(obs_action_sigma1) < len(obs_action_sigma2):\n",
    "#                     obs_action_sigma2 = obs_action_sigma2[1:]\n",
    "#                 else:\n",
    "#                     continue\n",
    "\n",
    "#                 X.append(np.concatenate((obs_action_sigma1, obs_action_sigma2), axis=1))\n",
    "#                 y.append(self.compare_probabilities(sigma1, sigma2))\n",
    "\n",
    "#         return X, y\n",
    "\n",
    "#     def setup_optimizers(self, optim_class, optim_kwargs):\n",
    "#         self.optim = optim_class(self.model.parameters(), **optim_kwargs)\n",
    "\n",
    "#     def _train_step(self, X, y):\n",
    "#         self.optim.zero_grad()\n",
    "#         loss = self._outer_step(X, y)\n",
    "#         loss.backward()\n",
    "#         self.optim.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     def _outer_step(self, X, y):\n",
    "#         outer_losses = []\n",
    "#         for i in tqdm(range(len(X))):\n",
    "#             if len(X[i])>self.num_support:\n",
    "#                 support_X, support_y, query_X, query_y = self._split_support_query(X[i], y[i])\n",
    "#                 # Inner loop (adaptation)\n",
    "#                 adapted_model = self._inner_loop(support_X, support_y)\n",
    "#                 # Compute loss using the adapted model on query set\n",
    "#                 query_loss = self._compute_loss(adapted_model, query_X, query_y)\n",
    "#                 outer_losses.append(query_loss)\n",
    "#         return torch.mean(torch.stack(outer_losses))\n",
    "\n",
    "#     def _inner_loop(self, support_X, support_y):\n",
    "#         adapted_model = Model(self.model.fc1.in_features, self.model.fc1.out_features,\n",
    "#                               self.model.fc2.out_features, self.model.fc3.out_features)\n",
    "#         adapted_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "#         inner_optimizer = optim.Adam(adapted_model.parameters(), lr=self.inner_lr)\n",
    "\n",
    "#         for _ in range(self.num_inner_steps):\n",
    "#             inner_optimizer.zero_grad()\n",
    "#             loss = self._compute_loss(adapted_model, support_X, support_y)\n",
    "#             print(loss)\n",
    "#             loss.backward()\n",
    "#             inner_optimizer.step()\n",
    "\n",
    "#         return adapted_model\n",
    "\n",
    "#     def _compute_loss(self, model, X, y):\n",
    "#         X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "#         y_class = [0 if y[i]==[1,0] else 1 for i in range(len(y))]\n",
    "#         y_tensor = torch.tensor(y_class, dtype=torch.long)  # Assuming y is class indices\n",
    "#         output = model(X_tensor)\n",
    "\n",
    "#         loss = self.reward_criterion(output, y_tensor)\n",
    "#         return loss\n",
    "\n",
    "#     def _split_support_query(self, X, y):\n",
    "#         num_samples = len(X)\n",
    "#         all_indices = np.arange(num_samples)\n",
    "#         # Randomly sample support indices\n",
    "#         support_indices = np.random.choice(num_samples, self.num_support, replace=False)\n",
    "#         query_indices = np.setdiff1d(all_indices, support_indices)\n",
    "#         support_X = X[support_indices]\n",
    "#         query_X = X[query_indices]\n",
    "#         # For y, we can simply use the same indices as for X, as it has a fixed length of 2\n",
    "#         support_y = [y] * self.num_support\n",
    "#         query_y = [y] * len(query_indices)\n",
    "\n",
    "#         return support_X, support_y, query_X, query_y\n",
    "\n",
    "\n",
    "# ml10 = data.copy()  \n",
    "# input_size = 86  # Assuming obs has 39 numbers and action has 4 numbers * 2 for pair of sigmas\n",
    "# hidden_size1 = 128\n",
    "# hidden_size2 = 128\n",
    "# output_size = 2\n",
    "# num_epochs = 5\n",
    "# outer_lr = 0.001\n",
    "\n",
    "# model = PreferenceMAML(ml10, input_size, hidden_size1, hidden_size2, output_size)\n",
    "# model.setup_optimizers(optim.Adam, {\"lr\": outer_lr})\n",
    "\n",
    "# print('Preparing Data.')\n",
    "# # X, y = model.prepare_data(k=4)\n",
    "# print('Data Preparation Done.\\n')\n",
    "\n",
    "# # Train the model\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f'\\nBeginning Training - Epoch [{epoch+1}/{num_epochs}]')\n",
    "#     loss = model._train_step(X, y)\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
