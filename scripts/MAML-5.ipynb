{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import sys\n",
    "import torch\n",
    "\n",
    "DATASETS = '../datasets/'\n",
    "\n",
    "def read_file(dir_name):\n",
    "    main_df = pd.DataFrame()\n",
    "    directory = DATASETS + dir_name\n",
    "    for filename in os.listdir(directory):\n",
    "        data = np.load(os.path.join(directory, filename))\n",
    "        data_dict = {}\n",
    "        for keys in data.keys():\n",
    "            data_dict[keys] = list(data[keys])\n",
    "        df = pd.DataFrame.from_dict(data_dict)\n",
    "        main_df = pd.concat([main_df, df])\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mw/cls_sweep-v2\n",
      "mw/cls_push-v2\n",
      "mw/cls_door-open-v2\n",
      "mw/cls_peg-insert-side-v2\n",
      "mw/cls_drawer-close-v2\n",
      "mw/cls_basketball-v2\n",
      "mw/cls_reach-v2\n",
      "mw/cls_window-open-v2\n",
      "mw/cls_pick-place-v2\n",
      "mw/cls_button_press_topdown-v2\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for name in os.listdir(\"../datasets/mw\"):\n",
    "    inner_df = pd.DataFrame()\n",
    "    if not (name.startswith('.')):\n",
    "        dir_name = 'mw/'+name\n",
    "        print(dir_name)\n",
    "        df = read_file(dir_name)\n",
    "        inner_df = pd.concat([inner_df, df])    \n",
    "    data.append(inner_df)\n",
    "data = np.array(data, dtype=object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class MetaRewardMLPEnsemble(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layers=[256, 256, 256],\n",
    "        # ensemble_size=3,\n",
    "        act=F.leaky_relu,\n",
    "        output_act=torch.tanh,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        params = {}\n",
    "        last_dim = 39 + 4\n",
    "        self.num_layers = len(hidden_layers) + 1\n",
    "        for i, dim in enumerate(\n",
    "            hidden_layers\n",
    "            + [\n",
    "                1,\n",
    "            ]\n",
    "        ):\n",
    "            weight = torch.empty(last_dim, dim)\n",
    "            weight.transpose_(0, 1)\n",
    "            nn.init.kaiming_uniform_(weight, a=math.sqrt(5))\n",
    "            weight.transpose_(0, 1)\n",
    "            params[f\"linear_w_{i}\"] = nn.Parameter(weight)\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(params[f\"linear_w_{i}\"].T)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            params[f\"linear_b_{i}\"] = nn.Parameter(\n",
    "                nn.init.uniform_(torch.empty(1, dim, requires_grad=True), -bound, bound)\n",
    "            )\n",
    "            last_dim = dim\n",
    "\n",
    "        self.params = nn.ParameterDict(params)\n",
    "        # self.parameters = self.params.parameters ######### NEW\n",
    "        # self.ensemble_size = ensemble_size\n",
    "        self.act = act\n",
    "        self.output_act = output_act\n",
    "\n",
    "    def forward(self, x, params=None):\n",
    "        if params is None:\n",
    "            params = self.params\n",
    "        # x = torch.cat((obs, action), dim=1)\n",
    "        # x = x.repeat(self.ensemble_size, 1, 1)\n",
    "        for i in range(self.num_layers):\n",
    "            # print('b_0: ', params[f\"linear_b_{i}\"].repeat(n_x, 1).shape)\n",
    "            # print('x:', x.shape)\n",
    "            # print('w_0: ', params['linear_w_0'].shape)\n",
    "            # x = torch.baddbmm(params[f\"linear_b_{i}\"], x, params[f\"linear_w_{i}\"])\n",
    "            # print(params)\n",
    "            x = params[f\"linear_b_{i}\"] + torch.matmul( x, params[f\"linear_w_{i}\"] )\n",
    "            if i == self.num_layers - 1:\n",
    "                x = self.output_act(x)\n",
    "            else:\n",
    "                x = self.act(x)\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size1=256, hidden_size2=256, hidden_size3=256, output_size = 2750):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "#         self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "#         self.out = nn.Linear(hidden_size2, 1)\n",
    "    \n",
    "#     # def forward_inner(self, x, parameters):\n",
    "#     #     x = torch.relu(self.fc1(x, parameters['fc1.weight'], parameters['fc1.bias']))\n",
    "#     #     x = torch.relu(self.fc2(x, parameters['fc2.weight'], parameters['fc2.bias']))\n",
    "#     #     x = torch.tanh(self.out(x, parameters['out.weight'], parameters['out.bias']))\n",
    "#     #     return x\n",
    "#     def forward(self, x):\n",
    "#         # print(x.shape)\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.tanh(self.out(x))\n",
    "#         return x\n",
    "    \n",
    "#     def para_forw(self, x, param):\n",
    "#         # print(param['fc1.weight'].shape)\n",
    "#         x = torch.relu(nn.functional.linear(x, param['fc1.weight'], param['fc1.bias']))\n",
    "#         x = torch.relu(nn.functional.linear(x, param['fc2.weight'], param['fc2.bias']))\n",
    "#         x = torch.tanh(nn.functional.linear(x, param['out.weight'], param['out.bias']))\n",
    "        # return x\n",
    "\n",
    "\n",
    "class PreferenceMAML:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ml10,\n",
    "        input_size,\n",
    "        hidden_size1,\n",
    "        hidden_size2,\n",
    "        outer_lr = 0.0001,\n",
    "        inner_lr = 0.001,\n",
    "        num_support=10,\n",
    "        num_query=10,\n",
    "        num_inner_steps=5,\n",
    "        k = 25,\n",
    "        num_tasks = 10,\n",
    "        episode_per_task = 1250,\n",
    "        output_size = 2750,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.ml10 = ml10\n",
    "        self.reward_criterion =  nn.BCELoss()\n",
    "        self.num_support = num_support\n",
    "        self.num_query = num_query\n",
    "        self.num_inner_steps = num_inner_steps\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        self.k = k\n",
    "        self.num_tasks = num_tasks\n",
    "        self.episode_per_task = episode_per_task\n",
    "        self.num_segments = None\n",
    "        self.model = MetaRewardMLPEnsemble()\n",
    "\n",
    "\n",
    "    def construct_episodes(self, ml10):\n",
    "        # episodes - n (tasks) x num_episode (each npz file)  , each cell is a dataframe of the episode \n",
    "        self.episodes = []\n",
    "        for task in ml10:\n",
    "            if(len(task)==0):\n",
    "                continue\n",
    "            task_episodes=[]\n",
    "            row_index = task[task['done'] == True].index.tolist()\n",
    "            prev=0\n",
    "            for x in row_index:\n",
    "                task_episodes.append(task[prev:x+1])\n",
    "                prev=x+1\n",
    "            task_episodes = np.array(task_episodes,dtype=object)\n",
    "            self.episodes.append(task_episodes)\n",
    "        self.episodes = np.array(self.episodes,dtype=object)\n",
    "        # return episodes\n",
    "\n",
    "    def form_sigma_groups(self, episode):\n",
    "        #num_segments = int(episode.shape[0] / self.k)\n",
    "        split_indices = np.arange(self.k, episode.shape[0], self.k)\n",
    "        # print(num_segments)\n",
    "        if len(split_indices) != 0:\n",
    "            l_segment = np.array_split(episode.iloc[::-1][:(self.k*(episode.shape[0] // self.k))], split_indices)\n",
    "            for i in range(len(l_segment)):\n",
    "                l_segment[i] = l_segment[i].iloc[::-1]\n",
    "            if(len(l_segment[-1])<25):\n",
    "                l_segment=l_segment[:-1]\n",
    "            return l_segment\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def compare_probabilities(self, sigma1, sigma2):\n",
    "        exp_sum_rewards_sigma1 = np.exp(sum(row['reward'] for row in sigma1))\n",
    "        exp_sum_rewards_sigma2 = np.exp(sum(row['reward'] for row in sigma2))\n",
    "        prob = exp_sum_rewards_sigma1 / (exp_sum_rewards_sigma1 + exp_sum_rewards_sigma2)\n",
    "        return [0] if prob > 0.5 else [1]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        X = []\n",
    "        y = []\n",
    "        episodes = self.episodes\n",
    "        # sigmas = self.form_sigma_groups(episodes, k)\n",
    "        sigmas = []\n",
    "        for task in episodes:\n",
    "            sigma = []\n",
    "            for episode in task:\n",
    "                segment = self.form_sigma_groups(episode)\n",
    "                # print(len(segment))\n",
    "                if segment is not None:\n",
    "                    sigma.append(segment)\n",
    "            # sigma = [self.form_sigma_groups(episode, k) for episode in task]\n",
    "            sigmas.append(sigma)\n",
    "        sigmas = np.array(sigmas, dtype=object)\n",
    "        all_lengths = [len(episode) for task in sigmas for episode in task]\n",
    "        self.num_segments = min(all_lengths)\n",
    "        if(self.num_segments<3):\n",
    "            self.num_segments=3\n",
    "        for task in sigmas:\n",
    "            task_list=[]\n",
    "            for episode in task:\n",
    "                ep_list=[]\n",
    "                if(len(episode)<self.num_segments):\n",
    "                    continue\n",
    "                for i in range(self.num_segments):\n",
    "                    y.append(episode[i][\"reward\"])\n",
    "                    ep_list.append(episode[i].drop('reward', axis=1))\n",
    "                task_list.append(ep_list)\n",
    "            task_list=np.array(task_list, dtype=object).reshape(-1,1) # convert row vector to col vector\n",
    "            X.append(task_list)\n",
    "        X_new=[]\n",
    "        task_counter = []\n",
    "        for task in X:\n",
    "            counter = 0\n",
    "            for i in range(0,len(task),4):\n",
    "                X_new.append(np.concatenate((task[i][0], task[i+1][0])))\n",
    "                counter += 1\n",
    "            task_counter.append(counter)\n",
    "        X = np.array(X_new, dtype=object)\n",
    "        task_counts = np.array(task_counter, dtype=object)\n",
    "        y=np.array(y,dtype=object)\n",
    "        y=y.flatten()\n",
    "        return X, y, task_counts\n",
    "    \n",
    "    def batchify(self, X, y, task_lengths, task_no, num=110):\n",
    "        # print(task_no)\n",
    "        if task_no == 0:\n",
    "            task_beg = 0  \n",
    "            task_end = task_lengths[0]\n",
    "        else:    \n",
    "            task_beg = np.sum(task_lengths[0:task_no])    \n",
    "            task_end = np.sum(task_lengths[0:task_no+1])\n",
    "        rand = random.sample(range(0,400), 110)\n",
    "        X_task = X[task_beg:task_end]\n",
    "        y_task = y[task_beg:task_end]\n",
    "        X_random = []\n",
    "        y_random = []\n",
    "        for r in rand:\n",
    "            X_random.append(X_task[r*25 : r*25+25])\n",
    "            y_random.append(y_task[r*25 : r*25+25])\n",
    "        X_random = np.array(X_random, dtype=np.float32)\n",
    "        y_random = np.array(y_random, dtype=np.float32)\n",
    "        return X_random.reshape(2750,43), y_random.reshape(2750,)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, y, task_lengths, num_epochs):\n",
    "        # optimizer = optim.Adam(self.model.params.values(), lr = self.outer_lr) ######### self.model.parameters() self.model.params.values()\n",
    "        # optimizer.zero_grad()\n",
    "        # self.optim[\"reward\"].zero_grad()\n",
    "        # print(self.model.state_dict())\n",
    "        # print(self.model.parameters())\n",
    "        plt_x = np.arange(0, num_epochs)\n",
    "        plt_y = np.zeros(num_epochs)\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            # optimizer.zero_grad()\n",
    "            loss = self._outer_step(self.model, X, y, task_lengths) \n",
    "            if loss is None:\n",
    "                return {}\n",
    "            print('loss from train:', loss)\n",
    "            plt_y[epoch] = loss\n",
    "            # print(\"Train Loss 1: \",plt_y[epoch])\n",
    "            # print(\"Train loss 2: \", loss)\n",
    "\n",
    "            #######################################\n",
    "            #######################################\n",
    "            grad = []\n",
    "            # print(new_dict.values().requires_grad)\n",
    "            for name, val in self.model.params.items():\n",
    "                print(name)\n",
    "                # val.requires_grad = True\n",
    "                # print(val.requires_grad)\n",
    "                grad.append(torch.autograd.grad(loss, val, create_graph=True))\n",
    "\n",
    "            print(grad) ####### Getting huge gradients in the 1st epoch which is I think why from the 2nd epoch onwards, getting 0 in gradients. CHECK OUTPUT BELOW\n",
    "\n",
    "            idx = 0\n",
    "            # new_dict = {}\n",
    "            for name, w in self.model.params.items():\n",
    "                print(name)\n",
    "                # print(grad[idx][0])\n",
    "                self.model.params[name] = w - self.outer_lr * grad[idx][0]\n",
    "                # raise Exception\n",
    "                idx += 1\n",
    "                # w.copy_(w_prime)\n",
    "                # new_dict[name] = w_prime\n",
    "            # return new_dict\n",
    "            #######################################\n",
    "            #######################################\n",
    "            \n",
    "\n",
    "            # if epoch%5 == 0:\n",
    "            #     plt.plot(plt_x[:epoch], plt_y[:epoch])\n",
    "            #     plt.xlabel(\"Epochs\")\n",
    "            #     plt.ylabel(\"Mean Loss\")\n",
    "            #     plt.show() \n",
    "            #     print(f\"Epoch: {epoch}, Loss: {plt_y[epoch]}\")\n",
    "            \n",
    "            # print(loss.grad)\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            # self.optim[\"reward\"].step()\n",
    "\n",
    "\n",
    "    def _outer_step(self, model, X, y, task_lengths):\n",
    "        outer_losses = []\n",
    "        for t in range(self.num_tasks):\n",
    "            X_b, y_b = self.batchify(X, y, task_lengths, t)\n",
    "            H, W = X_b.shape\n",
    "            inner_x = X_b[:H//2, :]\n",
    "            inner_y = y_b[:H//2]\n",
    "            outer_x = X_b[H//2:, :]\n",
    "            outer_y = y_b[H//2:]\n",
    "            weights = self._inner_loop(inner_x, inner_y, model = model)\n",
    "            # model_outer = copy.deepcopy(model)\n",
    "            # model_outer = Model(43, 256, 256, 256)\n",
    "            # model_outer.load_state_dict(weights)\n",
    "            # model_outer.load_state_dict(weights)\n",
    "            outer_losses.append(self._compute_loss(outer_x, outer_y, model, parameters=weights))\n",
    "        # print(outer_losses)\n",
    "        if (len(outer_losses) == 0):\n",
    "            return None\n",
    "        outer_loss = torch.mean(torch.stack(outer_losses))\n",
    "        # print(\"Outer Loss: \", loss)\n",
    "        return outer_loss\n",
    "\n",
    "\n",
    "            \n",
    "    def _inner_loop(self, X, y, model):\n",
    "        # parameters = {k: torch.clone(v) for k, v in model.ParameterDict()}\n",
    "        # model_inner = copy.deepcopy(model)\n",
    "        # opt_inner = optim.SGD(model_inner.parameters(), lr = self.inner_lr)\n",
    "        # state_dict = model.state_dict()\n",
    "        # print(id(model.parameters()))\n",
    "        # print(id(model.params))\n",
    "        # print(model.state_dict())\n",
    "        params_og = model.params\n",
    "        new_dict = {k: torch.clone(v) for k, v in params_og.items()} ######### state_dict\n",
    "        \n",
    "\n",
    "        # print(new_dict)                                                        ######### IN 2nd EPOCH SOMEHOW new_dict has values {new_dict from 1st epoch +/- 256}\n",
    "\n",
    "\n",
    "        loss = self._compute_loss(X, y, model, parameters = new_dict)\n",
    "        # print(\"Loss computed\")\n",
    "        # grad = torch.autograd.grad(loss, parameters, allow_unused=True)\n",
    "        # for name, w in model_inner.named_parameters():\n",
    "        #     print(name, w)\n",
    "        # print('loss from inner:',loss)\n",
    "\n",
    "        grad = []\n",
    "        # print(new_dict.values().requires_grad)\n",
    "        for val in new_dict.values():\n",
    "            # print(val)\n",
    "            # val.requires_grad = True\n",
    "            # print(val.requires_grad)\n",
    "            grad.append(torch.autograd.grad(loss, val, create_graph=True))\n",
    "            # print(\"Gradient\")\n",
    "            # print(grad)\n",
    "            # raise Exception\n",
    "        # print(\"Grad: \")\n",
    "        # print(grad)\n",
    "        # print(len(grad))\n",
    "        # raise Exception\n",
    "        # print(len(grad))\n",
    "        \n",
    "        # print(grad)\n",
    "        idx = 0\n",
    "        # new_dict = {}\n",
    "        for name, w in params_og.items():\n",
    "            # print(grad[idx][0])\n",
    "            new_dict[name] = w - self.inner_lr * grad[idx][0]\n",
    "            # raise Exception\n",
    "            idx += 1\n",
    "            # w.copy_(w_prime)\n",
    "            # new_dict[name] = w_prime\n",
    "        return new_dict\n",
    "        # for name, w in model_inner.named_parameters():\n",
    "        #     print(name, w)\n",
    "        # raise Exception\n",
    "        # weights = list(map(lambda p: p[1] - self.inner_lr * p[0], zip(grad, parameters)))\n",
    "        # print(weights)\n",
    "\n",
    "        # print(\"Inner loss: \", loss)\n",
    "        # opt_inner.zero_grad()\n",
    "        # loss.backward()\n",
    "        \n",
    "        # opt_inner.step()\n",
    "\n",
    "        # grads = torch.autograd.grad(loss, parameters, create_graph=True)\n",
    "        #for j,k in enumerate(parameters.keys()):\n",
    "         #   parameters[k] = parameters[k] - self.inner_lr * grads[j]\n",
    "        # for name, w in model_inner.named_parameters():\n",
    "        #     if 'weight' in name:\n",
    "        #         w = w - self.inner_lr * grads\n",
    "\n",
    "        # parameters = parameters - self.inner_lr * grads\n",
    "        # return weights\n",
    "\n",
    "    '''\n",
    "    R_E = \n",
    "    [\n",
    "        [\n",
    "            sigma_s1^E1, ..., sigma_s#^E1\n",
    "        ],\n",
    "        [\n",
    "            sigma_s1^E2, ..., sigma_s#^E2\n",
    "        ], ...,\n",
    "        [\n",
    "            sigma_s1^EN, ..., sigma_s#^EN\n",
    "        ]\n",
    "    ]\n",
    "    '''\n",
    "    #self.k = segment lenght\n",
    "    def _compute_loss(self, X, y, model, parameters = None):\n",
    "        # print(X, y, X.shape, y.shape)\n",
    "        # state_dict = model.state_dict()\n",
    "        # if parameters is not None:\n",
    "        #     model.load_state_dict(parameters)\n",
    "        \n",
    "        X_tensor = torch.from_numpy(X)\n",
    "        y_tensor = torch.from_numpy(y)\n",
    "        if parameters is not None:\n",
    "            output_reward = model.forward(X_tensor, parameters) # WORKING\n",
    "        else:\n",
    "            print('Params are NONE')\n",
    "            output_reward = model(X_tensor)\n",
    "        # N_o, _ = output_reward.shape\n",
    "        # print(output_reward.shape)\n",
    "        x =  y_tensor.shape[0]\n",
    "        N = x//self.k\n",
    "        output_reward = output_reward.reshape(N, self.k)\n",
    "        output_reward = torch.sum(output_reward, dim=1)\n",
    "        y_tensor = y_tensor.reshape(N, self.k)\n",
    "        y_tensor = torch.sum(y_tensor, dim=1)\n",
    "        loss = 0\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        #out_mat = np.exp(out_mat)\n",
    "        # out_logit = []\n",
    "        # out_y = []\n",
    "        loss = []\n",
    "        for i in range(N):\n",
    "            # sig_1 = output_reward[i]\n",
    "            # y_1 = y_tensor[i]\n",
    "            for j in range(i+1, N):\n",
    "                # sig_2 = output_reward[j]\n",
    "                # y_2 = y_tensor[j]\n",
    "                if y_tensor[i] > y_tensor[j]:\n",
    "                    loss.append(criterion(output_reward[j] - output_reward[i], torch.tensor(0.0, requires_grad= True)))\n",
    "                else:\n",
    "                    loss.append(criterion(output_reward[j] - output_reward[i], torch.tensor(1.0, requires_grad= True)))\n",
    "        loss = torch.sum(torch.stack(loss))\n",
    "        # if parameters is not None:\n",
    "        #     model.load_state_dict(state_dict)\n",
    "        return loss\n",
    " # for i in range(N):\n",
    "        #     # print(torch.sum(output_reward[i*self.k:(i+1)*self.k]).detach().numpy())\n",
    "        #     out_mat[i, :] = torch.sum(output_reward[i*self.k:(i+1)*self.k]).detach().numpy()\n",
    "        #     y_mat[i, :] = torch.sum(y_tensor[i*self.k:(i+1)*self.k]).detach().numpy()\n",
    "  # if (model is not None) and (parameters is not None) :\n",
    "        #     print(\"Errors\")\n",
    "        #     return\n",
    "        # if model is not None:  \n",
    "        #     output_reward = model(X_tensor)\n",
    "        # elif parameters is not None:\n",
    "        #     output_reward = self.model(X_tensor, parameters)\n",
    "        # else:\n",
    "        #     output_reward = self.model(X_tensor)\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "################################################################################################\n",
    "\n",
    "ml10 = data.copy()  \n",
    "input_size = 43  # Assuming obs has 39 numbers and action has 4 numbers * 2 for pair of sigmas\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 256\n",
    "hidden_size3 = 256\n",
    "# hidden_size3 = 1024\n",
    "# hidden_size4 = 2075\n",
    "\n",
    "output_size = 1\n",
    "num_epochs = 5\n",
    "outer_lr = 0.0001\n",
    "\n",
    "model = PreferenceMAML(ml10, input_size, hidden_size1, hidden_size2, hidden_size3)\n",
    "# model.setup_optimizers(optim.Adam, {\"lr\": outer_lr})\n",
    "\n",
    "model.construct_episodes(ml10)\n",
    "print('Preparing Data.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X, y, task_lengths = model.prepare_data()\n",
    "print('Data Preparation Done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss from train: tensor(2324.9971, grad_fn=<MeanBackward0>)\n",
      "linear_b_0\n",
      "linear_b_1\n",
      "linear_b_2\n",
      "linear_b_3\n",
      "linear_w_0\n",
      "linear_w_1\n",
      "linear_w_2\n",
      "linear_w_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▏                                         | 1/20 [01:27<27:44, 87.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([[ -68.7082,   72.0294, -576.3405,  -69.3016,  381.7574,   60.0491,\n",
      "          350.6154,  228.3223,  -49.3580,  -24.1033,  204.5439,  582.2731,\n",
      "         -112.4015,  215.2001, -339.8309,  286.2420, -207.1312,   80.6952,\n",
      "           65.6334, -108.5318,  -43.8447,  332.7116,  122.9751, -345.1053,\n",
      "          147.2433,  -13.2236,  112.5974,  104.1945,   82.9405, -272.2548,\n",
      "         -232.9718,   11.5025,  -54.5274,  -93.2794, -231.1078,   59.2271,\n",
      "          -38.3274, -174.8276,  251.6951,   42.3030, -203.6741,  176.8885,\n",
      "          -83.2413,  -91.3308,  290.2905,  -21.7983,    4.7904, -120.6527,\n",
      "         -176.3731,   94.8023, -406.1809,   34.5247,  -22.5598,  114.4927,\n",
      "          -25.8400,  -96.6176, -453.0181, -150.1973,  -28.1418, -102.3662,\n",
      "           47.3284,  126.9768,  162.9436,  212.2165, -103.4936,  356.3253,\n",
      "          -60.2857,  -89.0393,  229.6513,   -1.1514, -137.4643,  119.4581,\n",
      "         -155.9859,  330.3606,  346.5030,  115.4570,  187.0656,   45.9997,\n",
      "         -178.1470,  309.8925,  112.5930,  246.3219,   52.9416,  238.8277,\n",
      "         -102.4996,   16.6456,    4.4626,  299.3643,  -68.4622,  -25.6317,\n",
      "          -12.9804,  134.3820,   14.4722,  145.2726,  165.3511,  626.9245,\n",
      "          160.3364,  111.6262, -274.5236,   40.3008, -144.0268, -131.5768,\n",
      "           61.2792,  310.8427,  341.9012,   63.8121,  -89.3534,  158.6071,\n",
      "          366.6854, -355.2845,   -1.4446,   77.5114, -191.6253,  256.8665,\n",
      "           94.7663, -113.1490,  199.7999,  290.7019,  176.0849,  -10.0307,\n",
      "         -221.6458, -491.0404,  172.3780,  132.9074, -145.9300,  220.1370,\n",
      "           66.3646, -465.1072, -215.1710,  -14.7277,  -15.0824, -237.2980,\n",
      "          125.0321,   61.5297,  -33.2048,    3.3044, -144.4514,  345.0990,\n",
      "          204.5259, -142.8144,  403.1891,   96.4917,   86.1293,  204.8278,\n",
      "          184.6868,  236.1358,  -67.1930,  113.1541,   58.6010,  155.7740,\n",
      "         -137.1367, -451.1944,  129.9005,  225.3369,   82.5722,   15.9471,\n",
      "         -279.1381, -175.1598,  576.4474,  386.9568,  -98.5013, -164.0902,\n",
      "         -212.7957,  339.5703,  -77.8878,  -82.6663,  313.0922, -213.8054,\n",
      "           40.0947, -142.3707,  -51.9804, -341.5091, -179.1698,  434.5976,\n",
      "          273.2181,  395.4912,   39.9290, -363.9535,  160.1481, -342.8296,\n",
      "           51.3326,  -35.2299, -384.1880,  -21.0224,  147.3964,  -81.7081,\n",
      "         -173.3491,  126.8797, -115.5072,   38.2847,   -3.3814,   51.7984,\n",
      "          173.8667,  -36.6123,  388.1235, -149.5625,   13.7507,  -16.1861,\n",
      "          250.5380, -134.2131, -301.8735,   95.8544,  104.3537,  -42.4032,\n",
      "          294.6816,  -19.6644, -175.9583, -148.6402, -175.9080,  473.1482,\n",
      "         -174.3647,  143.3665, -192.7249,   48.8229, -240.7769,  107.9991,\n",
      "          -26.5834, -192.6214,  103.1318,   75.6990,  223.2058, -383.3000,\n",
      "         -379.7237,  142.3502, -375.0218, -215.1749,  142.4531,   54.3801,\n",
      "          -56.1888,  -53.3050,  136.1914, -168.6412, -403.7405,  280.1574,\n",
      "          141.1210,   85.3323, -227.9987,  -39.1378,  -15.8543, -188.4503,\n",
      "           19.4670,  -65.0570, -138.4384,  -29.8320,   39.1209,  -72.8108,\n",
      "          253.1918,  -41.9047, -274.2377, -119.4055, -155.9572, -143.5192,\n",
      "         -240.8069,  149.2227,  -36.6318,  221.9523]], grad_fn=<AddBackward0>),), (tensor([[ 7.6868e+01, -8.8162e+01, -2.5336e+02, -6.9255e+01, -9.0875e+01,\n",
      "          7.9155e+02,  1.4041e+01,  6.6794e+02,  6.8467e+02,  4.6114e+02,\n",
      "         -6.1158e+01, -9.4956e+02, -5.4757e+02,  6.7659e+02, -6.2802e+02,\n",
      "          5.4576e+02,  3.5747e+02,  1.6759e+02,  3.6440e+01,  1.1161e+03,\n",
      "         -1.2238e+03,  1.5948e+02, -8.8674e+02, -1.0063e+01, -5.2324e+02,\n",
      "          1.6516e+02,  5.8729e+02,  1.9601e+00, -1.5717e-01, -5.4172e+02,\n",
      "         -1.7374e+02, -3.6222e+02, -1.1065e+03,  2.4111e+02, -2.6162e+02,\n",
      "          5.0566e+02, -2.4624e+02, -2.1299e+02, -1.9179e+02, -3.2146e+02,\n",
      "         -1.8704e+02, -9.4158e+02, -2.6675e+02, -5.0334e+02,  1.9191e+02,\n",
      "         -1.7085e+01,  9.2794e+02, -4.1423e+00, -6.5111e+01,  2.1182e+01,\n",
      "          9.8569e+01, -2.2755e+02,  1.1726e+02, -1.9869e+02, -3.7979e+02,\n",
      "         -5.6923e+02, -9.4663e+01, -2.0099e+02,  1.3579e+03, -1.4122e+02,\n",
      "          1.2846e+02, -1.1024e+03,  8.8880e+02,  2.5691e+01,  3.0881e+02,\n",
      "          5.4019e+01,  1.4463e+02, -2.8700e+02, -2.0572e+00,  2.3254e+01,\n",
      "         -1.5713e+02, -5.0284e+01,  6.9886e+02, -2.1418e+02, -9.0621e+01,\n",
      "         -2.9341e+00, -4.1797e+02, -1.2825e+02,  6.0648e+02, -5.6604e+02,\n",
      "         -2.1558e+01, -3.9180e+02,  4.3461e+02, -4.7083e+00,  1.8309e+02,\n",
      "         -1.3621e+02,  6.3075e+02,  1.5704e+02, -4.4434e+01,  2.4593e+02,\n",
      "         -8.5900e+01, -1.7711e+02, -2.9775e+02, -8.0988e+00, -2.7453e+01,\n",
      "         -3.1635e+02,  2.7446e+01,  1.0132e+03,  1.5540e+01,  4.9075e-01,\n",
      "          2.9788e+02, -6.4723e+02,  1.0051e+02,  3.6657e+02,  9.0104e+02,\n",
      "         -8.6764e+02,  6.1602e+01,  5.6432e+02,  4.8655e+02,  2.2154e+02,\n",
      "         -7.6050e+01, -1.1745e+02, -2.0505e+02,  2.9845e+02, -1.1111e+03,\n",
      "          8.3845e+02, -2.0881e+01, -1.9497e+02,  1.3845e+01, -2.1134e+02,\n",
      "          6.7683e+01,  2.8223e+02,  9.8041e+02,  9.5309e+01,  1.8006e+02,\n",
      "         -2.7105e+02, -8.9585e+02, -7.6176e+02,  2.7130e+02,  7.2710e+02,\n",
      "         -5.3291e+02, -3.3780e+02,  7.7033e+02, -7.6902e+01, -7.7387e+01,\n",
      "          1.5169e+01,  9.1003e+01, -7.3247e+02, -1.1017e+03, -1.9004e+00,\n",
      "          2.1274e+01,  2.8655e+01, -1.3004e+02,  4.7891e+01, -9.1344e+02,\n",
      "          6.7708e+01, -2.6273e+02, -1.0665e+02,  7.5254e+02, -1.0304e+01,\n",
      "         -6.1443e+01, -1.1002e+02, -5.5591e+00,  2.5946e+02,  1.1550e+02,\n",
      "          3.8554e+02, -2.1486e+02,  2.7716e+02,  6.9403e+01,  1.3887e+02,\n",
      "          6.0983e+02,  4.4228e+02, -1.5917e+02, -6.5053e+01, -4.6864e+02,\n",
      "          6.0724e+02, -1.6583e+02,  4.3445e+01,  5.3878e+02,  1.6353e+02,\n",
      "         -1.8672e+02, -4.2238e+02, -2.7532e+01,  1.6368e+02, -3.1660e+02,\n",
      "         -6.8349e+01, -8.1173e+02, -3.9244e+02,  1.2043e+02, -2.8321e+02,\n",
      "         -6.0167e+02, -3.7797e+02,  2.3847e+02, -1.0083e+03, -9.5067e+01,\n",
      "         -9.6290e+02,  3.2057e+02, -5.8468e+00, -2.6994e+02, -1.7876e+02,\n",
      "         -1.5249e+02,  3.6191e+02,  7.4492e+00,  1.3633e+03, -2.7560e+02,\n",
      "         -1.0529e+03,  1.5832e+02, -6.6127e+01, -5.9504e+02,  7.4623e+01,\n",
      "          5.2106e+01, -1.3802e+02,  1.0799e+03, -2.2176e+02, -2.5933e+02,\n",
      "          3.7356e+02, -9.4957e+01, -6.5174e+02,  5.0386e+02,  1.5204e+03,\n",
      "          3.5474e+02, -2.5128e+02, -2.5628e+02, -4.1200e+02,  3.7729e+00,\n",
      "         -4.2975e+02, -9.1664e+01, -3.9083e+02, -5.2922e+01,  7.6060e+02,\n",
      "          1.2840e+02,  8.8349e+02,  1.1217e+03, -7.8272e+02,  4.7605e+01,\n",
      "          7.3498e+00, -3.8251e+02,  9.9042e+02,  1.4701e+02,  6.6126e+02,\n",
      "         -3.0202e+02,  2.9054e+02,  2.9872e+02,  1.0614e+02, -6.1063e+02,\n",
      "         -2.7629e+02,  2.9145e+02,  6.8207e+02,  5.9761e+02, -3.6786e+01,\n",
      "          1.1002e+01, -2.1220e+01,  1.1896e+02, -2.8846e+02, -2.7115e+02,\n",
      "         -3.0206e+02, -4.9711e+02, -1.7196e+02, -5.5290e+02,  3.4558e+02,\n",
      "         -2.3798e+01,  1.3641e+01, -5.9647e+02, -3.3545e+01,  1.2581e+02,\n",
      "          2.6881e+02]], grad_fn=<AddBackward0>),), (tensor([[ 1.2040e+02,  6.6184e+00,  3.8016e+02, -1.7513e+02,  8.9293e+02,\n",
      "         -1.4085e+02, -3.8572e+02,  9.6602e+02, -7.7832e-01, -4.5798e+02,\n",
      "          2.1519e+03,  4.4936e+01, -4.0165e+01,  1.7376e+03,  1.9380e+01,\n",
      "          1.9613e+02,  2.8447e+01, -9.5761e+02,  4.9148e+02, -1.6534e+03,\n",
      "         -3.2412e+02,  6.1005e+00,  2.1832e+03,  6.6162e+02,  1.0355e+02,\n",
      "         -9.5478e+02,  3.4940e+02,  5.2247e+01,  1.0329e+03,  2.8449e+03,\n",
      "         -2.6640e+02,  5.7634e+02, -5.0649e+02,  4.3939e+02,  1.9674e+03,\n",
      "          1.0105e+03, -4.1258e+02,  1.2777e+02, -1.2550e+02, -1.4963e+02,\n",
      "         -2.8788e+01,  2.1323e+03, -2.5381e+00,  4.6340e+02, -3.4221e+01,\n",
      "         -6.1304e+02, -1.3350e+03,  2.9000e+02, -1.0982e+02, -2.5679e+03,\n",
      "          1.5622e+03,  1.4166e+00, -7.9624e+02, -6.6331e+02,  2.2253e+00,\n",
      "          6.3021e+02,  1.2950e-01, -1.3800e+02, -2.3390e+02, -1.4402e+03,\n",
      "          1.5326e+00,  8.5419e+02, -3.3016e+01, -1.3109e+02,  1.4000e+03,\n",
      "          2.9638e+02, -7.5508e+01,  4.6146e+03,  2.8877e+03, -7.2178e+02,\n",
      "          1.5514e+02, -1.1318e+02,  1.8678e+02, -2.6769e+02,  7.4796e+01,\n",
      "         -9.3496e+02, -1.2714e+03, -1.3995e+03, -1.1503e-01,  6.3420e+02,\n",
      "          2.3086e+02, -2.3081e+03,  5.4629e+02,  4.1249e+03,  3.5414e+02,\n",
      "          5.1613e+02, -1.0829e+01, -5.8222e+02, -5.7311e+01,  8.5291e+00,\n",
      "          5.1543e+01,  1.6791e+03, -6.0533e+02,  4.4040e+02,  3.5125e+02,\n",
      "          1.0547e+02, -9.8408e+02,  4.3828e+01,  2.1725e+01,  2.5986e+03,\n",
      "         -3.7836e+03, -5.0393e+01,  4.2190e-02,  1.2023e+02,  4.5327e+01,\n",
      "          3.1848e+02,  1.5072e+01, -1.8032e+03,  9.5014e+01, -7.9776e+01,\n",
      "         -8.3414e+02,  1.2263e+03, -1.6338e+01,  1.8637e+01, -9.2474e+00,\n",
      "         -2.0644e+02, -9.9949e+02, -1.0256e+03, -1.2955e+03, -2.9143e+00,\n",
      "          6.3345e+02,  1.6810e+03,  2.9256e+02, -9.9622e+01,  3.6403e-01,\n",
      "          3.9493e+01, -8.1855e+02,  1.8955e+01,  2.9569e+02,  1.4852e+03,\n",
      "          7.0185e+02,  1.2197e+02, -9.0141e+02, -8.4765e+02,  2.3456e+02,\n",
      "          3.0221e+03, -1.0839e+03,  1.5557e+02,  1.7504e+03,  2.7162e-01,\n",
      "          3.0059e+00,  1.4852e+01,  5.0643e+00,  4.2244e+02, -4.5048e+02,\n",
      "          7.0035e+01, -2.4614e+02,  2.0062e+01, -2.4277e+00, -4.7015e+02,\n",
      "         -1.3573e+01,  3.2243e+02, -1.1648e+02,  1.2718e+03,  4.6705e+00,\n",
      "         -6.7065e+02,  3.4873e+02,  5.8268e+01, -1.3612e+01,  3.8300e+02,\n",
      "         -4.0758e+02, -7.9308e+02, -2.9670e+02,  5.1648e+02, -8.5887e+01,\n",
      "         -5.3903e+02, -3.4366e+01, -4.4836e+02, -1.5305e+03,  1.5295e+01,\n",
      "          2.5043e+02, -5.5114e+02, -1.4895e+01,  1.5245e+03, -1.4819e+03,\n",
      "         -3.0700e+02,  2.4351e+01,  2.4891e+00,  5.2481e+02, -1.3766e+02,\n",
      "          8.2210e+02, -7.0663e+01, -1.0308e+03,  1.1506e+03,  9.1711e+02,\n",
      "         -1.2332e+03,  1.0930e+02, -1.2282e+02, -1.3001e+02,  1.8685e+01,\n",
      "         -8.8945e+02,  2.1509e+03, -3.6805e+02,  2.5404e+01, -1.5211e+03,\n",
      "         -1.6375e+01,  8.6822e+02, -4.3443e+01, -5.7756e+01,  1.7102e+03,\n",
      "          1.5341e+03,  5.7636e+01, -1.0966e+02,  3.3195e+02,  2.7610e+02,\n",
      "         -7.6037e+01, -1.5476e+03, -2.3344e+02, -1.1473e+03,  4.7036e+02,\n",
      "         -3.7694e+03,  3.7876e+00, -8.0031e+01, -2.6752e+03,  9.4038e-01,\n",
      "         -1.2801e+01,  3.3954e+01,  3.1833e+03, -4.7952e+01, -9.7312e+01,\n",
      "         -9.1730e-02, -2.4492e+03,  8.2580e+01,  7.9743e+02,  4.6521e-01,\n",
      "          1.1659e+01, -4.3468e+00,  2.3689e+03, -3.4796e+02, -1.2792e+03,\n",
      "          8.4022e+01, -8.2438e+02, -1.0517e+03, -1.1373e+03, -6.6897e+02,\n",
      "         -4.3271e+01, -9.5711e+02, -1.3658e+02,  1.0318e+03,  1.3918e+01,\n",
      "         -4.1489e+02,  1.3619e+03, -2.8604e+02,  4.8009e+01, -5.8915e+02,\n",
      "          3.6513e-01, -2.9556e+02,  8.3737e+02,  2.0648e+03,  3.3506e+02,\n",
      "         -6.2791e+01,  7.3255e+01, -1.8281e+03, -5.9609e+00,  3.2261e+02,\n",
      "         -3.5410e+01]], grad_fn=<AddBackward0>),), (tensor([[1625.5205]], grad_fn=<AddBackward0>),), (tensor([[ 1.5095e+01,  3.6373e+00,  2.4356e+01,  ..., -4.6849e+00,\n",
      "         -5.2325e+00,  2.1407e+01],\n",
      "        [-3.7015e+01,  4.2685e+01, -3.6582e+02,  ...,  1.0079e+02,\n",
      "         -2.5416e+01,  1.3172e+02],\n",
      "        [-2.9745e+01,  1.3600e+01, -1.8186e+02,  ...,  4.9582e+01,\n",
      "         -1.1351e+01, -5.9124e-01],\n",
      "        ...,\n",
      "        [-2.9342e+02, -4.4195e+01, -3.0361e+02,  ..., -2.6419e+01,\n",
      "         -4.8633e+01,  2.0016e+02],\n",
      "        [ 9.5479e+01, -5.4431e+01,  1.5008e+03,  ..., -6.0225e+02,\n",
      "          3.2318e+02, -3.1851e+01],\n",
      "        [-8.5782e+01,  2.0320e+01, -3.0682e+02,  ...,  7.7239e+01,\n",
      "          1.9927e+01,  2.3916e+02]], grad_fn=<AddBackward0>),), (tensor([[-7.8987e+00, -7.3701e+01,  2.5780e+02,  ..., -3.2642e+01,\n",
      "         -2.4596e+00,  3.2411e+02],\n",
      "        [-3.3979e+00, -1.6015e+01,  2.4712e+00,  ..., -5.1717e-02,\n",
      "         -3.4009e+00,  1.9433e+01],\n",
      "        [-1.1270e+02, -2.1579e+01, -1.8374e+02,  ..., -1.0675e+01,\n",
      "          5.9435e+01, -8.1195e+00],\n",
      "        ...,\n",
      "        [-8.5750e+01, -1.1157e+01, -2.1415e+01,  ..., -1.2026e+00,\n",
      "         -8.4823e+00, -8.5695e+00],\n",
      "        [ 1.3015e+01, -1.2191e+00,  2.7943e+00,  ..., -7.8274e-01,\n",
      "         -3.8673e+01, -3.2363e+00],\n",
      "        [ 3.5145e+01, -6.2063e-01,  6.0981e+01,  ..., -2.6457e+00,\n",
      "         -1.7957e+02,  1.1073e+00]], grad_fn=<AddBackward0>),), (tensor([[-2.4377e+00,  6.7330e-01,  4.8789e-01,  ..., -2.8769e-01,\n",
      "          4.9324e+01, -7.0370e-01],\n",
      "        [-8.1286e+01, -2.6614e-01,  1.5147e+00,  ..., -1.1168e+00,\n",
      "          6.6949e+01, -2.4293e-01],\n",
      "        [-8.4315e+02, -1.2783e+00,  3.4658e+02,  ..., -1.0726e+01,\n",
      "          3.2352e+02, -9.2502e+00],\n",
      "        ...,\n",
      "        [-1.3293e+00,  3.2166e-02,  1.5989e-01,  ..., -7.8595e-03,\n",
      "         -1.7451e+00, -1.5093e+00],\n",
      "        [ 2.9032e+01,  2.4179e-01,  9.6591e+00,  ...,  4.3299e-01,\n",
      "         -3.7936e+00,  3.2242e-01],\n",
      "        [-1.4601e+02, -6.4773e-02,  2.0100e+01,  ..., -1.7513e+00,\n",
      "          1.0286e+02, -4.7451e+00]], grad_fn=<AddBackward0>),), (tensor([[-3.9348e+03],\n",
      "        [ 9.7564e+02],\n",
      "        [-1.3786e+03],\n",
      "        [ 1.0904e+03],\n",
      "        [-1.6616e+03],\n",
      "        [ 7.3302e+01],\n",
      "        [-2.0207e+02],\n",
      "        [-2.3478e+03],\n",
      "        [ 4.0366e+01],\n",
      "        [-2.0083e+01],\n",
      "        [-1.6258e+02],\n",
      "        [-3.3965e+03],\n",
      "        [-5.5052e+03],\n",
      "        [ 2.6679e+03],\n",
      "        [ 1.6916e+02],\n",
      "        [-1.0537e+03],\n",
      "        [ 6.6130e+01],\n",
      "        [-3.1202e+02],\n",
      "        [-1.8993e+02],\n",
      "        [-2.7616e+03],\n",
      "        [-4.5768e+03],\n",
      "        [ 1.2423e+02],\n",
      "        [-5.2299e+03],\n",
      "        [-5.6194e+02],\n",
      "        [-1.5878e+03],\n",
      "        [-5.1748e+03],\n",
      "        [-1.0893e+03],\n",
      "        [ 4.8252e+02],\n",
      "        [ 1.1809e+03],\n",
      "        [-2.9009e+02],\n",
      "        [-2.8537e+03],\n",
      "        [ 6.9734e+01],\n",
      "        [ 2.7237e+03],\n",
      "        [ 2.3988e+01],\n",
      "        [-3.9033e+03],\n",
      "        [-9.2035e+02],\n",
      "        [-7.8109e+02],\n",
      "        [ 2.3882e+03],\n",
      "        [ 4.5753e+01],\n",
      "        [-6.2727e+03],\n",
      "        [-2.7757e+03],\n",
      "        [ 1.8257e+03],\n",
      "        [ 7.1798e+00],\n",
      "        [ 1.7757e+03],\n",
      "        [-1.7763e+03],\n",
      "        [ 3.1391e+03],\n",
      "        [-3.0145e+03],\n",
      "        [-1.2848e+03],\n",
      "        [-1.6818e+02],\n",
      "        [ 1.7532e+03],\n",
      "        [ 2.3315e+03],\n",
      "        [ 9.5868e+01],\n",
      "        [ 2.9327e+03],\n",
      "        [ 7.1740e+02],\n",
      "        [ 3.8878e+01],\n",
      "        [-5.7832e+03],\n",
      "        [-6.2270e+03],\n",
      "        [-8.7273e+03],\n",
      "        [ 2.0318e+02],\n",
      "        [-1.3309e+03],\n",
      "        [ 9.6650e+01],\n",
      "        [ 1.7314e+03],\n",
      "        [-1.3405e+03],\n",
      "        [-4.8157e+01],\n",
      "        [-8.3347e+03],\n",
      "        [-5.5691e+02],\n",
      "        [-6.4829e+03],\n",
      "        [ 2.2516e+03],\n",
      "        [ 3.5636e+02],\n",
      "        [-6.2184e+02],\n",
      "        [-2.7792e+03],\n",
      "        [-1.9026e+03],\n",
      "        [-6.0736e+03],\n",
      "        [-2.1327e+02],\n",
      "        [ 8.2402e+01],\n",
      "        [-3.2827e+03],\n",
      "        [-3.4565e+03],\n",
      "        [ 1.8020e+03],\n",
      "        [ 9.8020e+01],\n",
      "        [-1.9504e+03],\n",
      "        [-1.7221e+03],\n",
      "        [ 1.6059e+03],\n",
      "        [ 2.2767e+02],\n",
      "        [ 3.4072e+03],\n",
      "        [-1.3946e+01],\n",
      "        [-1.0978e+03],\n",
      "        [ 9.0849e+01],\n",
      "        [ 1.0549e+02],\n",
      "        [ 2.3728e+02],\n",
      "        [ 9.1004e+01],\n",
      "        [-3.6306e+03],\n",
      "        [ 7.7814e+02],\n",
      "        [ 1.4913e+03],\n",
      "        [ 7.0339e+02],\n",
      "        [-1.6862e+03],\n",
      "        [-2.1067e+03],\n",
      "        [-1.0407e+03],\n",
      "        [-5.9950e+03],\n",
      "        [-4.4650e+03],\n",
      "        [-5.0650e+03],\n",
      "        [ 3.2805e+03],\n",
      "        [-2.1568e+03],\n",
      "        [ 9.0184e+01],\n",
      "        [-1.4891e+02],\n",
      "        [-1.1191e+04],\n",
      "        [ 9.9238e+02],\n",
      "        [ 4.9028e+01],\n",
      "        [-2.6578e+03],\n",
      "        [-3.0260e+03],\n",
      "        [-6.6048e+03],\n",
      "        [-9.7693e+02],\n",
      "        [-6.5989e+02],\n",
      "        [ 3.1126e+02],\n",
      "        [ 5.9784e+01],\n",
      "        [-9.5194e+02],\n",
      "        [ 4.7498e+02],\n",
      "        [-2.0823e+03],\n",
      "        [ 3.1166e+03],\n",
      "        [-1.7804e+02],\n",
      "        [ 1.1069e+02],\n",
      "        [-1.5147e+03],\n",
      "        [-3.0751e+03],\n",
      "        [-1.3330e+04],\n",
      "        [-8.1149e+03],\n",
      "        [ 4.6885e+01],\n",
      "        [ 6.0371e+01],\n",
      "        [-5.3542e+02],\n",
      "        [-1.2386e+04],\n",
      "        [ 7.6298e+02],\n",
      "        [-2.7237e+03],\n",
      "        [-8.9879e+02],\n",
      "        [ 1.1964e+03],\n",
      "        [-7.3062e+02],\n",
      "        [-1.2670e+04],\n",
      "        [-1.9779e+01],\n",
      "        [ 2.2822e+03],\n",
      "        [ 3.7939e+02],\n",
      "        [-8.5932e+01],\n",
      "        [ 1.5501e+03],\n",
      "        [ 3.3208e+01],\n",
      "        [-1.7860e+01],\n",
      "        [ 8.3856e+01],\n",
      "        [-1.0872e+01],\n",
      "        [-7.1461e+02],\n",
      "        [-2.7052e+03],\n",
      "        [-1.7474e+03],\n",
      "        [-1.1584e+04],\n",
      "        [ 7.2104e+01],\n",
      "        [ 1.1307e+02],\n",
      "        [ 2.4096e+03],\n",
      "        [ 6.1875e+01],\n",
      "        [-1.2675e+03],\n",
      "        [-4.6558e+03],\n",
      "        [-5.8798e+03],\n",
      "        [ 5.3289e+01],\n",
      "        [-6.2950e+03],\n",
      "        [ 3.5193e+03],\n",
      "        [-9.1714e+03],\n",
      "        [-5.7482e+02],\n",
      "        [ 1.3122e+03],\n",
      "        [-4.4352e+03],\n",
      "        [-8.0998e+03],\n",
      "        [-2.4039e+03],\n",
      "        [-7.9951e+02],\n",
      "        [-9.7246e+03],\n",
      "        [-4.4330e+02],\n",
      "        [-4.3745e+00],\n",
      "        [-9.1250e+03],\n",
      "        [ 1.0245e+03],\n",
      "        [ 1.5653e+02],\n",
      "        [-8.1125e+03],\n",
      "        [ 7.2334e+01],\n",
      "        [-1.6130e+03],\n",
      "        [-6.2678e+03],\n",
      "        [ 1.2671e+03],\n",
      "        [-9.6507e+02],\n",
      "        [-3.0877e+03],\n",
      "        [-1.3841e+02],\n",
      "        [-2.3714e+02],\n",
      "        [-3.2735e+03],\n",
      "        [-2.5264e+03],\n",
      "        [-6.0424e+03],\n",
      "        [-1.7438e+03],\n",
      "        [ 3.2852e+03],\n",
      "        [-9.3103e+03],\n",
      "        [-1.5685e+02],\n",
      "        [-4.5026e+03],\n",
      "        [ 6.6035e+02],\n",
      "        [ 1.5664e+03],\n",
      "        [-2.8130e+03],\n",
      "        [ 1.9367e+03],\n",
      "        [ 1.3788e+03],\n",
      "        [ 3.7726e+02],\n",
      "        [ 6.3848e+01],\n",
      "        [ 1.6862e+03],\n",
      "        [ 7.0480e+01],\n",
      "        [-1.1944e+03],\n",
      "        [-2.7962e+03],\n",
      "        [-5.3824e+03],\n",
      "        [ 2.8923e+03],\n",
      "        [ 2.4157e+03],\n",
      "        [-9.2108e+03],\n",
      "        [ 1.7502e+02],\n",
      "        [-1.1684e+03],\n",
      "        [-4.2030e+03],\n",
      "        [ 8.5376e+01],\n",
      "        [ 2.8801e+02],\n",
      "        [ 8.9156e+01],\n",
      "        [-2.3337e+03],\n",
      "        [ 1.3584e+03],\n",
      "        [ 6.5090e+03],\n",
      "        [ 5.0557e+01],\n",
      "        [ 1.0028e+03],\n",
      "        [ 3.3740e+03],\n",
      "        [ 5.8024e+01],\n",
      "        [-1.1645e+03],\n",
      "        [-7.5770e+03],\n",
      "        [-6.6359e+03],\n",
      "        [ 4.7586e+02],\n",
      "        [ 9.1961e+01],\n",
      "        [ 6.2394e+01],\n",
      "        [ 7.3314e+02],\n",
      "        [-1.3001e+02],\n",
      "        [-3.0470e+03],\n",
      "        [ 1.0137e+02],\n",
      "        [ 7.5553e+01],\n",
      "        [-3.4811e+03],\n",
      "        [ 1.5510e+03],\n",
      "        [-5.1799e+02],\n",
      "        [-3.3731e+03],\n",
      "        [ 7.0433e+01],\n",
      "        [-1.4111e+03],\n",
      "        [-9.3161e+02],\n",
      "        [-3.1785e+03],\n",
      "        [ 2.1480e+03],\n",
      "        [ 3.2340e+01],\n",
      "        [-1.4447e+02],\n",
      "        [-5.9879e+01],\n",
      "        [ 4.0768e+02],\n",
      "        [ 1.1119e+02],\n",
      "        [ 2.3037e+02],\n",
      "        [ 3.2747e+03],\n",
      "        [-6.5198e+03],\n",
      "        [-2.7368e+03],\n",
      "        [-1.9441e+03],\n",
      "        [ 7.6393e+01],\n",
      "        [-3.4974e+03],\n",
      "        [ 1.7276e+03],\n",
      "        [-1.5046e+03],\n",
      "        [ 3.4251e+03],\n",
      "        [-4.7880e+01],\n",
      "        [-1.5112e+02],\n",
      "        [-2.6785e+03],\n",
      "        [ 3.0276e+01],\n",
      "        [-3.1457e+03],\n",
      "        [-3.3653e+02]], grad_fn=<AddBackward0>),)]\n",
      "linear_b_0\n",
      "linear_b_1\n",
      "linear_b_2\n",
      "linear_b_3\n",
      "linear_w_0\n",
      "linear_w_1\n",
      "linear_w_2\n",
      "linear_w_3\n",
      "loss from train: tensor(1029.3239, grad_fn=<MeanBackward0>)\n",
      "linear_b_0\n",
      "linear_b_1\n",
      "linear_b_2\n",
      "linear_b_3\n",
      "linear_w_0\n",
      "linear_w_1\n",
      "linear_w_2\n",
      "linear_w_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 2/20 [03:15<29:49, 99.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<AddBackward0>),), (tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<AddBackward0>),), (tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<AddBackward0>),), (tensor([[0.]], grad_fn=<AddBackward0>),), (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<AddBackward0>),), (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<AddBackward0>),), (tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<AddBackward0>),), (tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>),)]\n",
      "linear_b_0\n",
      "linear_b_1\n",
      "linear_b_2\n",
      "linear_b_3\n",
      "linear_w_0\n",
      "linear_w_1\n",
      "linear_w_2\n",
      "linear_w_3\n",
      "loss from train: tensor(1029.3239, grad_fn=<MeanBackward0>)\n",
      "linear_b_0\n",
      "linear_b_1\n",
      "linear_b_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▎                                      | 2/20 [03:58<35:43, 119.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[276], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mtrain(X, y, task_lengths, \u001b[39m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[275], line 267\u001b[0m, in \u001b[0;36mPreferenceMAML.train\u001b[0;34m(self, X, y, task_lengths, num_epochs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[39mprint\u001b[39m(name)\n\u001b[1;32m    265\u001b[0m     \u001b[39m# val.requires_grad = True\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[39m# print(val.requires_grad)\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     grad\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(loss, val, create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m    269\u001b[0m \u001b[39mprint\u001b[39m(grad)\n\u001b[1;32m    272\u001b[0m idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(X, y, task_lengths, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame()\n",
    "for name in os.listdir(\"../datasets/mw_valid\"):\n",
    "    if not (name.startswith('.')):\n",
    "        dir_name = 'mw_valid/'+name\n",
    "        print(dir_name)\n",
    "        df = read_file(dir_name)\n",
    "        test = pd.concat([data, df])\n",
    "\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "Test = PreferenceMAML(test, input_size, hidden_size1, hidden_size2, output_size)\n",
    "test_X, test_y = Test.prepare_data(k=4)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "def evaluate_model(model, X, y):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X)):\n",
    "            X_tensor = torch.tensor(X[i], dtype=torch.float32)\n",
    "            output = model.model(X_tensor.unsqueeze(0))  \n",
    "            predictions.append(output.squeeze().numpy())  \n",
    "\n",
    "    preds = []\n",
    "    for _ in range(len(predictions)):\n",
    "        preds.append((np.array(predictions[_]).mean()))\n",
    "\n",
    "    pred_label = []\n",
    "    for i in range(len(preds)):\n",
    "        pred_label.append([0] if preds[i]>0.5 else [1])\n",
    "    \n",
    "    sum = 0\n",
    "    for _ in range(len(y)):\n",
    "        sum += pred_label[_]==y[_]\n",
    "    accuracy = sum/len(y)\n",
    "    return accuracy, pred_label\n",
    "\n",
    "test_accuracy, pred_labels = evaluate_model(model, test_X, test_y)\n",
    "print(f'\\nTest Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without INNER LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import pandas as pd\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "#         self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "#         self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.sigmoid(self.fc3(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class PreferenceMAML:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         ml10,\n",
    "#         input_size,\n",
    "#         hidden_size1,\n",
    "#         hidden_size2,\n",
    "#         output_size,\n",
    "#         num_support=10,\n",
    "#         num_query=10,\n",
    "#         num_inner_steps=5,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         self.ml10 = ml10\n",
    "#         self.reward_criterion =  nn.CrossEntropyLoss()\n",
    "#         self.num_support = num_support\n",
    "#         self.num_query = num_query\n",
    "#         self.num_inner_steps = num_inner_steps\n",
    "\n",
    "#         self.model = Model(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "#     def construct_episodes(self):\n",
    "#         episodes = []\n",
    "#         episode = []\n",
    "#         for _, row in self.ml10.iterrows():\n",
    "#             episode.append(row)\n",
    "#             if row['done']:\n",
    "#                 episodes.append(episode)\n",
    "#                 episode = []\n",
    "#         return episodes\n",
    "\n",
    "\n",
    "\n",
    "#     def form_sigma_groups(self, episode, k):\n",
    "#         sigmas = []\n",
    "#         segments = []\n",
    "#         q, r = divmod(len(episode), k)\n",
    "#         for i in range(k):\n",
    "#             segments.append(episode[i*q+min(i,r) : (i+1)*q+min(i+1,r)])\n",
    "\n",
    "#         for i in range(k):\n",
    "#             sigma_i = segments[i]\n",
    "#             for j in range(i+1, k):\n",
    "#                 sigma_j = segments[j]\n",
    "\n",
    "#                 sigmas.append((sigma_i, sigma_j))\n",
    "#         return sigmas\n",
    "\n",
    "#     def compare_probabilities(self, sigma1, sigma2):\n",
    "#         exp_sum_rewards_sigma1 = np.exp(sum(row['reward'] for row in sigma1))\n",
    "#         exp_sum_rewards_sigma2 = np.exp(sum(row['reward'] for row in sigma2))\n",
    "#         prob = exp_sum_rewards_sigma1 / (exp_sum_rewards_sigma1 + exp_sum_rewards_sigma2)\n",
    "#         return [1,0] if prob > 0.5 else [0,1]\n",
    "\n",
    "\n",
    "#     def prepare_data(self, k):\n",
    "#         X = []\n",
    "#         y = []\n",
    "#         episodes = self.construct_episodes()\n",
    "#         for episode in episodes:\n",
    "#             sigmas = self.form_sigma_groups(episode, k)\n",
    "#             for _ in range(len(sigmas)):\n",
    "\n",
    "#                 sigma1 = sigmas[_][0]\n",
    "#                 sigma2 = sigmas[_][1]\n",
    "\n",
    "#                 obs_action_sigma1 = []\n",
    "#                 for row in sigma1:\n",
    "#                     obs_action = list(row['obs']) + list(row['action'])  # Concatenate obs and action\n",
    "#                     obs_action_sigma1.append(obs_action)\n",
    "\n",
    "#                 obs_action_sigma2 = []\n",
    "#                 for row in sigma2:\n",
    "#                     obs_action = list(row['obs']) + list(row['action'])  # Concatenate obs and action\n",
    "#                     obs_action_sigma2.append(obs_action)\n",
    "\n",
    "#                 if len(obs_action_sigma1) > len(obs_action_sigma2):\n",
    "#                     obs_action_sigma1 = obs_action_sigma1[1:]\n",
    "#                 elif len(obs_action_sigma1) < len(obs_action_sigma2):\n",
    "#                     obs_action_sigma2 = obs_action_sigma2[1:]\n",
    "#                 else:\n",
    "#                     continue\n",
    "\n",
    "#                 X.append(np.concatenate((obs_action_sigma1, obs_action_sigma2), axis = 1))\n",
    "#                 y.append([self.compare_probabilities(sigma1, sigma2)]) \n",
    "\n",
    "#         return X, y\n",
    "\n",
    "\n",
    "#     def setup_optimizers(self, optim_class, optim_kwargs):\n",
    "#         self.optim = optim_class(self.model.parameters(), **optim_kwargs)\n",
    "\n",
    "#     def _train_step(self, X, y):\n",
    "#         self.optim.zero_grad()\n",
    "#         loss = self._outer_step(X, y)\n",
    "#         loss.backward()\n",
    "#         self.optim.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     def _outer_step(self, X, y):\n",
    "#         outer_losses = []\n",
    "#         for i in range(len(X)):\n",
    "#             loss = self._compute_loss(X[i], y[i])\n",
    "#             outer_losses.append(loss)\n",
    "#         return torch.mean(torch.stack(outer_losses))\n",
    "\n",
    "#     def _compute_loss(self, X, y):\n",
    "#         X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "#         y_tensor = torch.tensor([y], dtype=torch.float32)\n",
    "#         output = self.model(X_tensor)\n",
    "#         output_flat = output.view(-1)\n",
    "#         y_flat = y_tensor.view(-1)\n",
    "#         loss = self.reward_criterion(output_flat[-2:], y_flat)\n",
    "#         return loss\n",
    "\n",
    "# ml10 = data.copy()  \n",
    "# input_size = 86  # Assuming obs has 39 numbers and action has 4 numbers * 2 for pair of sigmas\n",
    "# hidden_size1 = 128\n",
    "# hidden_size2 = 128\n",
    "# output_size = 2\n",
    "# num_epochs = 20\n",
    "\n",
    "# model = PreferenceMAML(ml10, input_size, hidden_size1, hidden_size2, output_size)\n",
    "# model.setup_optimizers(optim.Adam, {\"lr\": 0.005})\n",
    "\n",
    "# X, y = model.prepare_data(k=4)\n",
    "\n",
    "# # Train the model\n",
    "# for epoch in range(num_epochs):\n",
    "#     loss = model._train_step(X, y)\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With INNER LOOP but Improper classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "#         self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "#         self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = torch.sigmoid(self.fc3(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class PreferenceMAML:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         ml10,\n",
    "#         input_size,\n",
    "#         hidden_size1,\n",
    "#         hidden_size2,\n",
    "#         output_size,\n",
    "#         inner_lr = 0.01,\n",
    "#         num_support=10,\n",
    "#         num_query=10,\n",
    "#         num_inner_steps=5,\n",
    "#         **kwargs,\n",
    "#     ):\n",
    "#         self.ml10 = ml10\n",
    "#         self.reward_criterion =  nn.CrossEntropyLoss()\n",
    "#         self.num_support = num_support\n",
    "#         self.num_query = num_query\n",
    "#         self.num_inner_steps = num_inner_steps\n",
    "#         self.inner_lr = inner_lr\n",
    "\n",
    "#         self.model = Model(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "#     def construct_episodes(self):\n",
    "#         episodes = []\n",
    "#         episode = []\n",
    "#         for _, row in self.ml10.iterrows():\n",
    "#             episode.append(row)\n",
    "#             if row['done']:\n",
    "#                 episodes.append(episode)\n",
    "#                 episode = []\n",
    "#         return episodes\n",
    "\n",
    "#     def form_sigma_groups(self, episode, k):\n",
    "#         sigmas = []\n",
    "#         segments = []\n",
    "#         q, r = divmod(len(episode), k)\n",
    "#         for i in range(k):\n",
    "#             segments.append(episode[i*q+min(i,r) : (i+1)*q+min(i+1,r)])\n",
    "\n",
    "#         for i in range(k):\n",
    "#             sigma_i = segments[i]\n",
    "#             for j in range(i+1, k):\n",
    "#                 sigma_j = segments[j]\n",
    "\n",
    "#                 sigmas.append((sigma_i, sigma_j))\n",
    "#         return sigmas\n",
    "\n",
    "#     def compare_probabilities(self, sigma1, sigma2):\n",
    "#         exp_sum_rewards_sigma1 = np.exp(sum(row['reward'] for row in sigma1))\n",
    "#         exp_sum_rewards_sigma2 = np.exp(sum(row['reward'] for row in sigma2))\n",
    "#         prob = exp_sum_rewards_sigma1 / (exp_sum_rewards_sigma1 + exp_sum_rewards_sigma2)\n",
    "#         return [1,0] if prob > 0.5 else [0,1]\n",
    "\n",
    "#     def prepare_data(self, k):\n",
    "#         X = []\n",
    "#         y = []\n",
    "#         episodes = self.construct_episodes()\n",
    "#         for episode in episodes:\n",
    "#             sigmas = self.form_sigma_groups(episode, k)\n",
    "#             for _ in range(len(sigmas)):\n",
    "#                 sigma1 = sigmas[_][0]\n",
    "#                 sigma2 = sigmas[_][1]\n",
    "\n",
    "#                 obs_action_sigma1 = []\n",
    "#                 for row in sigma1:\n",
    "#                     obs_action = list(row['obs']) + list(row['action'])  # Concatenate obs and action\n",
    "#                     obs_action_sigma1.append(obs_action)\n",
    "\n",
    "#                 obs_action_sigma2 = []\n",
    "#                 for row in sigma2:\n",
    "#                     obs_action = list(row['obs']) + list(row['action'])  # Concatenate obs and action\n",
    "#                     obs_action_sigma2.append(obs_action)\n",
    "\n",
    "#                 if len(obs_action_sigma1) > len(obs_action_sigma2):\n",
    "#                     obs_action_sigma1 = obs_action_sigma1[1:]\n",
    "#                 elif len(obs_action_sigma1) < len(obs_action_sigma2):\n",
    "#                     obs_action_sigma2 = obs_action_sigma2[1:]\n",
    "#                 else:\n",
    "#                     continue\n",
    "\n",
    "#                 X.append(np.concatenate((obs_action_sigma1, obs_action_sigma2), axis=1))\n",
    "#                 y.append(self.compare_probabilities(sigma1, sigma2))\n",
    "\n",
    "#         return X, y\n",
    "\n",
    "#     def setup_optimizers(self, optim_class, optim_kwargs):\n",
    "#         self.optim = optim_class(self.model.parameters(), **optim_kwargs)\n",
    "\n",
    "#     def _train_step(self, X, y):\n",
    "#         self.optim.zero_grad()\n",
    "#         loss = self._outer_step(X, y)\n",
    "#         loss.backward()\n",
    "#         self.optim.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     def _outer_step(self, X, y):\n",
    "#         outer_losses = []\n",
    "#         for i in tqdm(range(len(X))):\n",
    "#             if len(X[i])>self.num_support:\n",
    "#                 support_X, support_y, query_X, query_y = self._split_support_query(X[i], y[i])\n",
    "#                 # Inner loop (adaptation)\n",
    "#                 adapted_model = self._inner_loop(support_X, support_y)\n",
    "#                 # Compute loss using the adapted model on query set\n",
    "#                 query_loss = self._compute_loss(adapted_model, query_X, query_y)\n",
    "#                 outer_losses.append(query_loss)\n",
    "#         return torch.mean(torch.stack(outer_losses))\n",
    "\n",
    "#     def _inner_loop(self, support_X, support_y):\n",
    "#         adapted_model = Model(self.model.fc1.in_features, self.model.fc1.out_features,\n",
    "#                               self.model.fc2.out_features, self.model.fc3.out_features)\n",
    "#         adapted_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "#         inner_optimizer = optim.Adam(adapted_model.parameters(), lr=self.inner_lr)\n",
    "\n",
    "#         for _ in range(self.num_inner_steps):\n",
    "#             inner_optimizer.zero_grad()\n",
    "#             loss = self._compute_loss(adapted_model, support_X, support_y)\n",
    "#             print(loss)\n",
    "#             loss.backward()\n",
    "#             inner_optimizer.step()\n",
    "\n",
    "#         return adapted_model\n",
    "\n",
    "#     def _compute_loss(self, model, X, y):\n",
    "#         X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "#         y_class = [0 if y[i]==[1,0] else 1 for i in range(len(y))]\n",
    "#         y_tensor = torch.tensor(y_class, dtype=torch.long)  # Assuming y is class indices\n",
    "#         output = model(X_tensor)\n",
    "\n",
    "#         loss = self.reward_criterion(output, y_tensor)\n",
    "#         return loss\n",
    "\n",
    "#     def _split_support_query(self, X, y):\n",
    "#         num_samples = len(X)\n",
    "#         all_indices = np.arange(num_samples)\n",
    "#         # Randomly sample support indices\n",
    "#         support_indices = np.random.choice(num_samples, self.num_support, replace=False)\n",
    "#         query_indices = np.setdiff1d(all_indices, support_indices)\n",
    "#         support_X = X[support_indices]\n",
    "#         query_X = X[query_indices]\n",
    "#         # For y, we can simply use the same indices as for X, as it has a fixed length of 2\n",
    "#         support_y = [y] * self.num_support\n",
    "#         query_y = [y] * len(query_indices)\n",
    "\n",
    "#         return support_X, support_y, query_X, query_y\n",
    "\n",
    "\n",
    "# ml10 = data.copy()  \n",
    "# input_size = 86  # Assuming obs has 39 numbers and action has 4 numbers * 2 for pair of sigmas\n",
    "# hidden_size1 = 128\n",
    "# hidden_size2 = 128\n",
    "# output_size = 2\n",
    "# num_epochs = 5\n",
    "# outer_lr = 0.001\n",
    "\n",
    "# model = PreferenceMAML(ml10, input_size, hidden_size1, hidden_size2, output_size)\n",
    "# model.setup_optimizers(optim.Adam, {\"lr\": outer_lr})\n",
    "\n",
    "# print('Preparing Data.')\n",
    "# # X, y = model.prepare_data(k=4)\n",
    "# print('Data Preparation Done.\\n')\n",
    "\n",
    "# # Train the model\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f'\\nBeginning Training - Epoch [{epoch+1}/{num_epochs}]')\n",
    "#     loss = model._train_step(X, y)\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
