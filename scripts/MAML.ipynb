{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import sys\n",
    "import torch\n",
    "\n",
    "DATASETS = '../datasets/'\n",
    "\n",
    "def read_file(dir_name):\n",
    "    main_df = pd.DataFrame()\n",
    "    directory = DATASETS + dir_name\n",
    "    for filename in os.listdir(directory):\n",
    "        data = np.load(os.path.join(directory, filename))\n",
    "        data_dict = {}\n",
    "        for keys in data.keys():\n",
    "            data_dict[keys] = list(data[keys])\n",
    "        df = pd.DataFrame.from_dict(data_dict)\n",
    "        # main_df = pd.concat([main_df, df])\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dir_name = 'mw/cls_basketball-v2'\n",
    "    # print([name for name in os.listdir(\"../datasets/mw\") if os.path.isdir(name)])\n",
    "    read_file(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_file(dir_name)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>done</th>\n",
       "      <th>discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>[0.04225361, 0.6922199, 0.3393161, 0.6348106, ...</td>\n",
       "      <td>[0.20169806, 0.09580077, 0.34795704, 0.57062256]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>[0.03985697, 0.70180416, 0.34516203, 0.6245957...</td>\n",
       "      <td>[-0.092127666, 0.016195506, 0.04491708, 0.509507]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>[0.041954525, 0.69937974, 0.34240845, 0.627575...</td>\n",
       "      <td>[0.04885615, -0.13805023, 0.20181242, 0.6775277]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>[0.0426317, 0.6936216, 0.34391972, 0.6367297, ...</td>\n",
       "      <td>[0.079129174, -0.046790846, 0.15598248, 0.6076...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>[0.045894675, 0.6963733, 0.34360257, 0.635374,...</td>\n",
       "      <td>[0.08643522, -0.062440854, 0.2643433, 0.6480871]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>[0.038542774, 0.69685125, 0.3427321, 0.6403711...</td>\n",
       "      <td>[0.12917, 0.014268942, 0.23423716, 0.53333265]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>[0.046851985, 0.69781697, 0.34231716, 0.630933...</td>\n",
       "      <td>[-0.109134324, 0.047157288, 0.18217964, 0.6325...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>[0.045193467, 0.6921859, 0.34096196, 0.628385,...</td>\n",
       "      <td>[0.032395396, -0.05911544, 0.3193123, 0.7464293]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>[0.045409724, 0.6938885, 0.3422801, 0.62775636...</td>\n",
       "      <td>[0.038899023, 0.13717479, 0.46352172, 0.5921459]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>[0.040127266, 0.69905657, 0.34206805, 0.624083...</td>\n",
       "      <td>[-0.08430083, -0.023767652, 0.39234892, 0.4676...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>[0.048386812, 0.7020242, 0.34401628, 0.6330884...</td>\n",
       "      <td>[-0.0068158987, 0.11303027, 0.11535144, 0.628131]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>[0.041853737, 0.6977879, 0.34120065, 0.6422756...</td>\n",
       "      <td>[0.06611314, -0.14851102, 0.23104869, 0.57119244]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>[0.047701977, 0.6976209, 0.3457508, 0.63503003...</td>\n",
       "      <td>[-0.040522218, -0.083348244, 0.15965997, 0.467...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>[0.044384405, 0.69647515, 0.34390083, 0.629587...</td>\n",
       "      <td>[-0.22230737, 0.16964957, 0.30552047, 0.67216897]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>[0.043199603, 0.6926576, 0.3414667, 0.63639337...</td>\n",
       "      <td>[0.19675353, -0.03503137, 0.14197162, 0.71333826]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>[0.09811697, 0.7001386, 0.33964658, 0.63458616...</td>\n",
       "      <td>[-0.059603598, -0.06458613, 0.29196653, 0.5428...</td>\n",
       "      <td>6.966027</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>[-0.03971396, 0.702369, 0.34451577, 0.6330972,...</td>\n",
       "      <td>[0.04506601, 0.063122146, 0.33856505, 0.6634487]</td>\n",
       "      <td>6.910524</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>[-0.08247734, 0.6948504, 0.339405, 0.627, -0.0...</td>\n",
       "      <td>[-0.06791842, -0.003114563, 0.27855745, 0.5275...</td>\n",
       "      <td>6.470166</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>[-0.080973595, 0.74979496, 0.3493339, 0.636784...</td>\n",
       "      <td>[0.038258266, 3.5086565, -0.026267888, 0.52791...</td>\n",
       "      <td>6.733230</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2180</th>\n",
       "      <td>[0.06668944, 0.7187602, 0.3552407, 0.6228304, ...</td>\n",
       "      <td>[-0.11672381, 4.263366, -0.3350737, 0.5848897]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>[-0.04005615, 0.7405593, 0.35015413, 0.6406026...</td>\n",
       "      <td>[0.0089841, 3.6547248, -0.13646717, 0.46512255]</td>\n",
       "      <td>6.993975</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2395</th>\n",
       "      <td>[-0.02661301, 0.6943251, 0.3429718, 0.63805383...</td>\n",
       "      <td>[-0.050319012, 0.013969061, 0.20838816, 0.5961...</td>\n",
       "      <td>6.913923</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>[-0.083147146, 0.7460646, 0.34964958, 0.632477...</td>\n",
       "      <td>[0.07703773, 3.3907576, 0.0126382345, 0.43083456]</td>\n",
       "      <td>6.688766</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>[-0.056442358, 0.72290057, 0.35140735, 0.63148...</td>\n",
       "      <td>[0.25720182, 3.7576735, -0.114540435, 0.5347301]</td>\n",
       "      <td>6.868070</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>[0.018526392, 0.6963037, 0.3448415, 0.63312787...</td>\n",
       "      <td>[-0.16599642, -0.16993552, 0.15074718, 0.527479]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>[-0.04258668, 0.7023568, 0.34252408, 0.6268092...</td>\n",
       "      <td>[-0.020181417, -0.08306205, 0.31290847, 0.4470...</td>\n",
       "      <td>6.897214</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>[-0.08532182, 0.7554468, 0.35276395, 0.6224364...</td>\n",
       "      <td>[-0.08992968, 3.0851898, -0.17802164, 0.7487823]</td>\n",
       "      <td>6.652472</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3103</th>\n",
       "      <td>[0.013186041, 0.7114386, 0.3543667, 0.63855475...</td>\n",
       "      <td>[0.041385807, 4.348377, -0.29023847, 0.66617626]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>[-0.086328134, 0.7450539, 0.35093075, 0.636861...</td>\n",
       "      <td>[-0.00017406656, 3.5123274, -0.10027066, 0.519...</td>\n",
       "      <td>6.650654</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344</th>\n",
       "      <td>[-0.01729717, 0.72327423, 0.35196573, 0.630552...</td>\n",
       "      <td>[0.12893042, 3.962985, -0.050230138, 0.53517634]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3457</th>\n",
       "      <td>[0.04421069, 0.6972948, 0.3445748, 0.6371326, ...</td>\n",
       "      <td>[0.019070221, 0.20704097, 0.32347038, 0.66581357]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>[0.012972263, 0.695614, 0.3434753, 0.63461536,...</td>\n",
       "      <td>[-0.04510722, -0.04247319, 0.030793617, 0.5755...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3688</th>\n",
       "      <td>[0.09249771, 0.7364713, 0.35231745, 0.63847435...</td>\n",
       "      <td>[0.27613294, 3.670895, -0.061828192, 0.75649345]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>[0.08800715, 0.73233086, 0.35542467, 0.6221125...</td>\n",
       "      <td>[-0.30786034, 3.822772, -0.24368173, 0.45647952]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>[0.009615374, 0.6994584, 0.34507495, 0.6359855...</td>\n",
       "      <td>[0.07402097, 0.082405716, 0.059864853, 0.7111885]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>[0.010608042, 0.6974703, 0.34192923, 0.6395842...</td>\n",
       "      <td>[0.027104363, 0.033820294, 0.3083466, 0.6982137]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4129</th>\n",
       "      <td>[0.045804977, 0.6954197, 0.34070882, 0.6370748...</td>\n",
       "      <td>[-0.05160152, -0.21103632, 0.2153985, 0.6768651]</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>[-0.024956593, 0.72501445, 0.3544038, 0.631819...</td>\n",
       "      <td>[-0.039126165, 3.7784002, -0.20711792, 0.6703609]</td>\n",
       "      <td>6.993462</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>[0.09936514, 0.733694, 0.35530645, 0.6296589, ...</td>\n",
       "      <td>[-0.23095639, 3.5770764, 0.042034402, 0.65892905]</td>\n",
       "      <td>6.993303</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4472</th>\n",
       "      <td>[-0.08436444, 0.70473367, 0.34138805, 0.633506...</td>\n",
       "      <td>[0.14933597, -0.08780122, 0.18018146, 0.6919952]</td>\n",
       "      <td>6.442314</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553</th>\n",
       "      <td>[-0.061482653, 0.68640274, 0.116827644, 0.2968...</td>\n",
       "      <td>[0.19494842, 0.6256187, -0.102002, 0.9381156]</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4627</th>\n",
       "      <td>[0.4825951, 0.627192, 0.098184526, 0.52649367,...</td>\n",
       "      <td>[0.8621412, -0.11143433, -0.122746244, -0.9095...</td>\n",
       "      <td>0.005275</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>[0.035566505, 0.82563823, 0.38942996, 0.293434...</td>\n",
       "      <td>[0.15739675, -0.007401907, -1.1600691, 0.9647788]</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4821</th>\n",
       "      <td>[0.09492427, 0.7289065, 0.3535554, 0.62959975,...</td>\n",
       "      <td>[-0.30533403, 3.8533278, -0.17157626, 0.5779038]</td>\n",
       "      <td>6.997565</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4866</th>\n",
       "      <td>[0.013707182, 0.69753003, 0.23518713, 0.689680...</td>\n",
       "      <td>[0.14371362, 0.21129379, 0.067276664, -0.01164...</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4935</th>\n",
       "      <td>[-0.042045336, 0.8467326, 0.39776167, 0.330962...</td>\n",
       "      <td>[-0.15125439, -0.24291113, -1.2469801, 1.016761]</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5021</th>\n",
       "      <td>[0.009770371, 0.8230807, 0.07742291, 0.2967751...</td>\n",
       "      <td>[1.9218056, -0.040443692, -0.03886999, 0.98951...</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5127</th>\n",
       "      <td>[0.48316538, 0.6242773, 0.10258975, 0.5225139,...</td>\n",
       "      <td>[0.8302178, 0.23314285, 0.021767177, -0.9737922]</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>[0.017310841, 0.69793826, 0.23799205, 0.683865...</td>\n",
       "      <td>[-0.009368679, -0.04379536, 0.23618206, -0.099...</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5273</th>\n",
       "      <td>[-0.029129945, 0.68662363, 0.1139359, 0.296879...</td>\n",
       "      <td>[0.12551266, 0.60658205, 0.016665418, 0.87124974]</td>\n",
       "      <td>0.091181</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    obs  \\\n",
       "107   [0.04225361, 0.6922199, 0.3393161, 0.6348106, ...   \n",
       "217   [0.03985697, 0.70180416, 0.34516203, 0.6245957...   \n",
       "318   [0.041954525, 0.69937974, 0.34240845, 0.627575...   \n",
       "429   [0.0426317, 0.6936216, 0.34391972, 0.6367297, ...   \n",
       "541   [0.045894675, 0.6963733, 0.34360257, 0.635374,...   \n",
       "660   [0.038542774, 0.69685125, 0.3427321, 0.6403711...   \n",
       "767   [0.046851985, 0.69781697, 0.34231716, 0.630933...   \n",
       "877   [0.045193467, 0.6921859, 0.34096196, 0.628385,...   \n",
       "977   [0.045409724, 0.6938885, 0.3422801, 0.62775636...   \n",
       "1085  [0.040127266, 0.69905657, 0.34206805, 0.624083...   \n",
       "1195  [0.048386812, 0.7020242, 0.34401628, 0.6330884...   \n",
       "1301  [0.041853737, 0.6977879, 0.34120065, 0.6422756...   \n",
       "1413  [0.047701977, 0.6976209, 0.3457508, 0.63503003...   \n",
       "1513  [0.044384405, 0.69647515, 0.34390083, 0.629587...   \n",
       "1618  [0.043199603, 0.6926576, 0.3414667, 0.63639337...   \n",
       "1728  [0.09811697, 0.7001386, 0.33964658, 0.63458616...   \n",
       "1836  [-0.03971396, 0.702369, 0.34451577, 0.6330972,...   \n",
       "1943  [-0.08247734, 0.6948504, 0.339405, 0.627, -0.0...   \n",
       "2067  [-0.080973595, 0.74979496, 0.3493339, 0.636784...   \n",
       "2180  [0.06668944, 0.7187602, 0.3552407, 0.6228304, ...   \n",
       "2294  [-0.04005615, 0.7405593, 0.35015413, 0.6406026...   \n",
       "2395  [-0.02661301, 0.6943251, 0.3429718, 0.63805383...   \n",
       "2515  [-0.083147146, 0.7460646, 0.34964958, 0.632477...   \n",
       "2639  [-0.056442358, 0.72290057, 0.35140735, 0.63148...   \n",
       "2747  [0.018526392, 0.6963037, 0.3448415, 0.63312787...   \n",
       "2858  [-0.04258668, 0.7023568, 0.34252408, 0.6268092...   \n",
       "2982  [-0.08532182, 0.7554468, 0.35276395, 0.6224364...   \n",
       "3103  [0.013186041, 0.7114386, 0.3543667, 0.63855475...   \n",
       "3224  [-0.086328134, 0.7450539, 0.35093075, 0.636861...   \n",
       "3344  [-0.01729717, 0.72327423, 0.35196573, 0.630552...   \n",
       "3457  [0.04421069, 0.6972948, 0.3445748, 0.6371326, ...   \n",
       "3567  [0.012972263, 0.695614, 0.3434753, 0.63461536,...   \n",
       "3688  [0.09249771, 0.7364713, 0.35231745, 0.63847435...   \n",
       "3797  [0.08800715, 0.73233086, 0.35542467, 0.6221125...   \n",
       "3909  [0.009615374, 0.6994584, 0.34507495, 0.6359855...   \n",
       "4020  [0.010608042, 0.6974703, 0.34192923, 0.6395842...   \n",
       "4129  [0.045804977, 0.6954197, 0.34070882, 0.6370748...   \n",
       "4247  [-0.024956593, 0.72501445, 0.3544038, 0.631819...   \n",
       "4368  [0.09936514, 0.733694, 0.35530645, 0.6296589, ...   \n",
       "4472  [-0.08436444, 0.70473367, 0.34138805, 0.633506...   \n",
       "4553  [-0.061482653, 0.68640274, 0.116827644, 0.2968...   \n",
       "4627  [0.4825951, 0.627192, 0.098184526, 0.52649367,...   \n",
       "4700  [0.035566505, 0.82563823, 0.38942996, 0.293434...   \n",
       "4821  [0.09492427, 0.7289065, 0.3535554, 0.62959975,...   \n",
       "4866  [0.013707182, 0.69753003, 0.23518713, 0.689680...   \n",
       "4935  [-0.042045336, 0.8467326, 0.39776167, 0.330962...   \n",
       "5021  [0.009770371, 0.8230807, 0.07742291, 0.2967751...   \n",
       "5127  [0.48316538, 0.6242773, 0.10258975, 0.5225139,...   \n",
       "5191  [0.017310841, 0.69793826, 0.23799205, 0.683865...   \n",
       "5273  [-0.029129945, 0.68662363, 0.1139359, 0.296879...   \n",
       "\n",
       "                                                 action     reward  done  \\\n",
       "107    [0.20169806, 0.09580077, 0.34795704, 0.57062256]  10.000000  True   \n",
       "217   [-0.092127666, 0.016195506, 0.04491708, 0.509507]  10.000000  True   \n",
       "318    [0.04885615, -0.13805023, 0.20181242, 0.6775277]  10.000000  True   \n",
       "429   [0.079129174, -0.046790846, 0.15598248, 0.6076...  10.000000  True   \n",
       "541    [0.08643522, -0.062440854, 0.2643433, 0.6480871]  10.000000  True   \n",
       "660      [0.12917, 0.014268942, 0.23423716, 0.53333265]  10.000000  True   \n",
       "767   [-0.109134324, 0.047157288, 0.18217964, 0.6325...  10.000000  True   \n",
       "877    [0.032395396, -0.05911544, 0.3193123, 0.7464293]  10.000000  True   \n",
       "977    [0.038899023, 0.13717479, 0.46352172, 0.5921459]  10.000000  True   \n",
       "1085  [-0.08430083, -0.023767652, 0.39234892, 0.4676...  10.000000  True   \n",
       "1195  [-0.0068158987, 0.11303027, 0.11535144, 0.628131]  10.000000  True   \n",
       "1301  [0.06611314, -0.14851102, 0.23104869, 0.57119244]  10.000000  True   \n",
       "1413  [-0.040522218, -0.083348244, 0.15965997, 0.467...  10.000000  True   \n",
       "1513  [-0.22230737, 0.16964957, 0.30552047, 0.67216897]  10.000000  True   \n",
       "1618  [0.19675353, -0.03503137, 0.14197162, 0.71333826]  10.000000  True   \n",
       "1728  [-0.059603598, -0.06458613, 0.29196653, 0.5428...   6.966027  True   \n",
       "1836   [0.04506601, 0.063122146, 0.33856505, 0.6634487]   6.910524  True   \n",
       "1943  [-0.06791842, -0.003114563, 0.27855745, 0.5275...   6.470166  True   \n",
       "2067  [0.038258266, 3.5086565, -0.026267888, 0.52791...   6.733230  True   \n",
       "2180     [-0.11672381, 4.263366, -0.3350737, 0.5848897]  10.000000  True   \n",
       "2294    [0.0089841, 3.6547248, -0.13646717, 0.46512255]   6.993975  True   \n",
       "2395  [-0.050319012, 0.013969061, 0.20838816, 0.5961...   6.913923  True   \n",
       "2515  [0.07703773, 3.3907576, 0.0126382345, 0.43083456]   6.688766  True   \n",
       "2639   [0.25720182, 3.7576735, -0.114540435, 0.5347301]   6.868070  True   \n",
       "2747   [-0.16599642, -0.16993552, 0.15074718, 0.527479]  10.000000  True   \n",
       "2858  [-0.020181417, -0.08306205, 0.31290847, 0.4470...   6.897214  True   \n",
       "2982   [-0.08992968, 3.0851898, -0.17802164, 0.7487823]   6.652472  True   \n",
       "3103   [0.041385807, 4.348377, -0.29023847, 0.66617626]  10.000000  True   \n",
       "3224  [-0.00017406656, 3.5123274, -0.10027066, 0.519...   6.650654  True   \n",
       "3344   [0.12893042, 3.962985, -0.050230138, 0.53517634]  10.000000  True   \n",
       "3457  [0.019070221, 0.20704097, 0.32347038, 0.66581357]  10.000000  True   \n",
       "3567  [-0.04510722, -0.04247319, 0.030793617, 0.5755...  10.000000  True   \n",
       "3688   [0.27613294, 3.670895, -0.061828192, 0.75649345]  10.000000  True   \n",
       "3797   [-0.30786034, 3.822772, -0.24368173, 0.45647952]  10.000000  True   \n",
       "3909  [0.07402097, 0.082405716, 0.059864853, 0.7111885]  10.000000  True   \n",
       "4020   [0.027104363, 0.033820294, 0.3083466, 0.6982137]  10.000000  True   \n",
       "4129   [-0.05160152, -0.21103632, 0.2153985, 0.6768651]  10.000000  True   \n",
       "4247  [-0.039126165, 3.7784002, -0.20711792, 0.6703609]   6.993462  True   \n",
       "4368  [-0.23095639, 3.5770764, 0.042034402, 0.65892905]   6.993303  True   \n",
       "4472   [0.14933597, -0.08780122, 0.18018146, 0.6919952]   6.442314  True   \n",
       "4553      [0.19494842, 0.6256187, -0.102002, 0.9381156]   0.000037  True   \n",
       "4627  [0.8621412, -0.11143433, -0.122746244, -0.9095...   0.005275  True   \n",
       "4700  [0.15739675, -0.007401907, -1.1600691, 0.9647788]   0.004966  True   \n",
       "4821   [-0.30533403, 3.8533278, -0.17157626, 0.5779038]   6.997565  True   \n",
       "4866  [0.14371362, 0.21129379, 0.067276664, -0.01164...   0.002455  True   \n",
       "4935   [-0.15125439, -0.24291113, -1.2469801, 1.016761]   0.004871  True   \n",
       "5021  [1.9218056, -0.040443692, -0.03886999, 0.98951...   0.013629  True   \n",
       "5127   [0.8302178, 0.23314285, 0.021767177, -0.9737922]   0.018520  True   \n",
       "5191  [-0.009368679, -0.04379536, 0.23618206, -0.099...   0.002568  True   \n",
       "5273  [0.12551266, 0.60658205, 0.016665418, 0.87124974]   0.091181  True   \n",
       "\n",
       "      discount  \n",
       "107        1.0  \n",
       "217        1.0  \n",
       "318        1.0  \n",
       "429        1.0  \n",
       "541        1.0  \n",
       "660        1.0  \n",
       "767        1.0  \n",
       "877        1.0  \n",
       "977        1.0  \n",
       "1085       1.0  \n",
       "1195       1.0  \n",
       "1301       1.0  \n",
       "1413       1.0  \n",
       "1513       1.0  \n",
       "1618       1.0  \n",
       "1728       1.0  \n",
       "1836       1.0  \n",
       "1943       1.0  \n",
       "2067       1.0  \n",
       "2180       1.0  \n",
       "2294       1.0  \n",
       "2395       1.0  \n",
       "2515       1.0  \n",
       "2639       1.0  \n",
       "2747       1.0  \n",
       "2858       1.0  \n",
       "2982       1.0  \n",
       "3103       1.0  \n",
       "3224       1.0  \n",
       "3344       1.0  \n",
       "3457       1.0  \n",
       "3567       1.0  \n",
       "3688       1.0  \n",
       "3797       1.0  \n",
       "3909       1.0  \n",
       "4020       1.0  \n",
       "4129       1.0  \n",
       "4247       1.0  \n",
       "4368       1.0  \n",
       "4472       1.0  \n",
       "4553       1.0  \n",
       "4627       1.0  \n",
       "4700       1.0  \n",
       "4821       1.0  \n",
       "4866       1.0  \n",
       "4935       1.0  \n",
       "5021       1.0  \n",
       "5127       1.0  \n",
       "5191       1.0  \n",
       "5273       1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.done==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6941280961036682\n",
      "Epoch 2, Loss: 0.673733651638031\n",
      "Epoch 3, Loss: 0.6542643904685974\n",
      "Epoch 4, Loss: 0.6355992555618286\n",
      "Epoch 5, Loss: 0.6176444888114929\n",
      "Epoch 6, Loss: 0.600238561630249\n",
      "Epoch 7, Loss: 0.5830855965614319\n",
      "Epoch 8, Loss: 0.5663089752197266\n",
      "Epoch 9, Loss: 0.549755871295929\n",
      "Epoch 10, Loss: 0.5332930684089661\n",
      "Epoch 11, Loss: 0.5169082880020142\n",
      "Epoch 12, Loss: 0.500662088394165\n",
      "Epoch 13, Loss: 0.48463568091392517\n",
      "Epoch 14, Loss: 0.4688534736633301\n",
      "Epoch 15, Loss: 0.4533584713935852\n",
      "Epoch 16, Loss: 0.43828192353248596\n",
      "Epoch 17, Loss: 0.42373624444007874\n",
      "Epoch 18, Loss: 0.40986621379852295\n",
      "Epoch 19, Loss: 0.39680245518684387\n",
      "Epoch 20, Loss: 0.38462498784065247\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class YourModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(YourModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class YourDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class PreferenceMAML:\n",
    "    def __init__(\n",
    "        self,\n",
    "        ml10,\n",
    "        input_size,\n",
    "        hidden_size1,\n",
    "        hidden_size2,\n",
    "        output_size,\n",
    "        num_support=10,\n",
    "        num_query=10,\n",
    "        num_inner_steps=5,\n",
    "        inner_lr=0.1,\n",
    "        learn_inner_lr=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.inner_lr = inner_lr\n",
    "        self.learn_inner_lr = learn_inner_lr\n",
    "        self.ml10 = ml10\n",
    "        self.reward_criterion = nn.BCELoss()\n",
    "        self.num_support = num_support\n",
    "        self.num_query = num_query\n",
    "        self.num_inner_steps = num_inner_steps\n",
    "\n",
    "        self.model = YourModel(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "    def construct_episodes(self):\n",
    "        episodes = []\n",
    "        episode = []\n",
    "        for _, row in self.ml10.iterrows():\n",
    "            episode.append(row)\n",
    "            if row['done']:\n",
    "                episodes.append(episode)\n",
    "                episode = []\n",
    "        return episodes\n",
    "\n",
    "    def form_sigma_groups(self, episode):\n",
    "        sigmas = []\n",
    "        for i in range(len(episode) - 1):\n",
    "            sigma1 = episode[:i+1]\n",
    "            sigma2 = episode[i+1:]\n",
    "            sigmas.append((sigma1, sigma2))\n",
    "        return sigmas\n",
    "\n",
    "    def compare_probabilities(self, sigma1, sigma2):\n",
    "        exp_sum_rewards_sigma1 = np.exp(sum(row['reward'] for row in sigma1))\n",
    "        exp_sum_rewards_sigma2 = np.exp(sum(row['reward'] for row in sigma2))\n",
    "        prob = exp_sum_rewards_sigma1 / (exp_sum_rewards_sigma1 + exp_sum_rewards_sigma2)\n",
    "        return [0,1] if prob > 0.5 else [1,0]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        X = []\n",
    "        y = []\n",
    "        episodes = self.construct_episodes()\n",
    "        for episode in episodes:\n",
    "            sigmas = self.form_sigma_groups(episode)\n",
    "            for sigma1, sigma2 in sigmas:\n",
    "                obs_action_sigma1 = []\n",
    "                for row in sigma1:\n",
    "                    obs_action = list(row['obs']) + list(row['action'])  # Concatenate obs and action\n",
    "                    obs_action_sigma1.append(obs_action)\n",
    "                X.append(obs_action_sigma1)\n",
    "                # Target value (y) for each sigma comparison\n",
    "                y.append([self.compare_probabilities(sigma1, sigma2)])  # Wrap in a list to match output shape\n",
    "        return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def setup_optimizers(self, optim_class, optim_kwargs):\n",
    "        self.optim = optim_class(self.model.parameters(), **optim_kwargs)\n",
    "\n",
    "    def _train_step(self, X, y):\n",
    "        self.optim.zero_grad()\n",
    "        loss = self._outer_step(X, y)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def _outer_step(self, X, y):\n",
    "        outer_losses = []\n",
    "        for i in range(len(X)):\n",
    "            loss = self._compute_loss(X[i], y[i])\n",
    "            outer_losses.append(loss)\n",
    "        return torch.mean(torch.stack(outer_losses))\n",
    "\n",
    "    def _compute_loss(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor([y], dtype=torch.float32)\n",
    "        output = self.model(X_tensor)\n",
    "\n",
    "        output_flat = output.view(-1)\n",
    "        y_flat = y_tensor.view(-1)\n",
    "\n",
    "        loss = self.reward_criterion(output_flat[-2:], y_flat)\n",
    "        return loss\n",
    "\n",
    "# Example usage\n",
    "ml10 = data.copy()  # Assume ml10 is a DataFrame with obs, action, reward, done columns\n",
    "input_size = 43  # Assuming obs has 39 numbers and action has 4 numbers\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 128\n",
    "output_size = 2\n",
    "num_epochs = 20\n",
    "model = PreferenceMAML(ml10, input_size, hidden_size1, hidden_size2, output_size)\n",
    "model.setup_optimizers(optim.Adam, {\"lr\": 0.001})\n",
    "\n",
    "# Prepare data\n",
    "X, y = model.prepare_data()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    loss = model._train_step(X, y)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def preprocess_df(df, num_segments):\n",
    "# df = data.copy()\n",
    "# num_segments = 4\n",
    "\n",
    "# episodes = []\n",
    "# current_episode = []\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     current_episode.append(row)\n",
    "#     if row['done'] or index == len(df) - 1:\n",
    "#         D = []\n",
    "#         segments = np.array_split(current_episode, num_segments)\n",
    "#         for i in range(len(segments) - 1):\n",
    "#             for j in range(i, len(segments) - 1):\n",
    "#                 sigma1 = pd.DataFrame(segments[i])\n",
    "#                 sigma2 = pd.DataFrame(segments[j])\n",
    "#                 reward_sum_sigma1 = sum(sigma1[2])\n",
    "#                 reward_sum_sigma2 = sum(sigma2[2])\n",
    "#                 p = torch.tensor([[np.exp(reward_sum_sigma1) / (np.exp(reward_sum_sigma1) + np.exp(reward_sum_sigma2))]])\n",
    "#                 y = torch.tensor([1]) if p.item() >= 0.5 else torch.tensor([0])\n",
    "\n",
    "#                 s1_obs = torch.tensor(sigma1[0])\n",
    "#                 s1_act = torch.tensor(sigma1[1])\n",
    "#                 s1_reward = torch.tensor(sigma1[2])\n",
    "\n",
    "#                 s2_obs = torch.tensor(sigma2[0])\n",
    "#                 s2_act = torch.tensor(sigma2[1])\n",
    "#                 s2_reward = torch.tensor(sigma2[2])\n",
    "\n",
    "                \n",
    "                \n",
    "#                 d = (p, y)\n",
    "#                 D.append(d)\n",
    "\n",
    "#                 # x_obs = torch.tensor(sigma1[0].tolist())\n",
    "#                 # x_act = torch.tensor(sigma1[1].tolist())\n",
    "#                 # y_support = torch.tensor([[1, 0]]) if p.item() > 0.5 else torch.tensor([[0, 1]])\n",
    "#                 # x_query = torch.tensor(sigma2[0].tolist())\n",
    "#                 # y_query = torch.tensor([0]) if p.item() > 0.5 else torch.tensor([1])\n",
    "#                 # episodes.append(((x_obs, x_act), y_support, (x_obs, x_act), y_query))\n",
    "#         episodes.append(D)\n",
    "#         current_episode = []\n",
    "\n",
    "#     # return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Define your neural network architecture\n",
    "# class Policy(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(Policy, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "#         self.double()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# # Define MAML algorithm\n",
    "# def maml(epochs_inner, epochs_outer, episodes, alpha_inner, beta_outer, num_inner_updates):\n",
    "#     # Initialize parameters\n",
    "#     input_size =  1 # specify the input size of your neural network\n",
    "#     output_size = 2 # specify the output size of your neural network\n",
    "    \n",
    "#     # Initialize policy network\n",
    "#     policy_net = Policy(input_size, 128, output_size)\n",
    "    \n",
    "#     # Initialize optimizer\n",
    "#     optimizer_outer = optim.Adam(policy_net.parameters(), lr=beta_outer)\n",
    "    \n",
    "#     # Define loss function\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     # Outer loop\n",
    "#     for epoch_outer in range(epochs_outer):\n",
    "        \n",
    "#         # Initialize meta-gradients\n",
    "#         meta_gradients = []\n",
    "        \n",
    "#         # Inner loop\n",
    "#         for episode in episodes:\n",
    "#             # Clone policy network\n",
    "#             policy_net_clone = Policy(input_size, 128, output_size)\n",
    "#             policy_net_clone.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "#             # Initialize inner optimizer\n",
    "#             optimizer_inner = optim.Adam(policy_net_clone.parameters(), lr=alpha_inner)\n",
    "            \n",
    "#             # Inner loop updates\n",
    "#             for _ in range(num_inner_updates):\n",
    "#                 input_data, target_data = ([episode[i][0] for i in range(len(episode))], [episode[i][1] for i in range(len(episode))])\n",
    "                \n",
    "#                 predictions = policy_net_clone(torch.tensor(np.array(input_data)))\n",
    "#                 predictions = predictions.view(-1, predictions.shape[-1])\n",
    "\n",
    "#                 loss = criterion(predictions, torch.tensor(target_data))\n",
    "                \n",
    "#                 optimizer_inner.zero_grad()\n",
    "\n",
    "\n",
    "#                 loss.backward()\n",
    "#                 optimizer_inner.step()\n",
    "            \n",
    "#             # Compute gradient of loss w.r.t. initial parameters\n",
    "#             print(torch.autograd.grad(loss, policy_net.parameters(), allow_unused=False))\n",
    "#             meta_gradients.append(torch.autograd.grad(loss, policy_net.parameters(), allow_unused=False))\n",
    "#             print(meta_gradients)\n",
    "        \n",
    "#         # Update policy network using meta-gradients\n",
    "#         mean_gradients = np.mean(meta_gradients, axis=0)\n",
    "#         for param, grad in zip(policy_net.parameters(), mean_gradients):\n",
    "#             param.data -= beta_outer * grad[0]\n",
    "        \n",
    "#         # Print loss after outer loop\n",
    "#         print(f'Epoch {epoch_outer+1}/{epochs_outer}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "# # Define hyperparameters\n",
    "# epochs_inner = 5\n",
    "# epochs_outer = 100\n",
    "# alpha_inner = 0.01\n",
    "# beta_outer = 0.001\n",
    "# num_inner_updates = 100\n",
    "\n",
    "# # Run MAML\n",
    "# model = maml(epochs_inner, epochs_outer, episodes, alpha_inner, beta_outer, num_inner_updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the reward function network\n",
    "# class RewardFunction(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim):\n",
    "#         super(RewardFunction, self).__init__()\n",
    "#         self.fc1 = nn.Linear(state_dim + action_dim, 64)\n",
    "#         self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "#     def forward(self, state, action):\n",
    "#         x = torch.cat([state, action], dim=1)\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Define the preference predictor\n",
    "# class PreferencePredictor(nn.Module):\n",
    "#     def __init__(self, state_dim, action_dim):\n",
    "#         super(PreferencePredictor, self).__init__()\n",
    "#         self.fc1 = nn.Linear(state_dim + action_dim, 64)\n",
    "#         self.fc2 = nn.Linear(64, 1)\n",
    "    \n",
    "#     def forward(self, state1, action1, state2, action2):\n",
    "#         x1 = torch.cat([state1, action1], dim=1)\n",
    "#         x2 = torch.cat([state2, action2], dim=1)\n",
    "#         x = torch.sigmoid(self.fc1(x1) - self.fc1(x2))\n",
    "#         return x\n",
    "\n",
    "# # Define the MAML algorithm\n",
    "# def maml_update(reward_net, preference_predictor, task_data, alpha, beta):\n",
    "#     reward_net_copy = RewardFunction(state_dim, action_dim)\n",
    "#     reward_net_copy.load_state_dict(reward_net.state_dict())\n",
    "#     optimizer = optim.Adam(reward_net_copy.parameters(), lr=alpha)\n",
    "    \n",
    "#     for _ in range(num_inner_updates):\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = preference_loss(reward_net_copy, preference_predictor, task_data)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     outer_loss = preference_loss(reward_net_copy, preference_predictor, task_data)\n",
    "#     reward_net_parameters = reward_net.parameters()\n",
    "#     outer_grad = torch.autograd.grad(outer_loss, reward_net_parameters, create_graph=True, allow_unused=True)\n",
    "#     for param, grad in zip(reward_net_parameters, outer_grad):\n",
    "#         param.data -= beta * grad\n",
    "    \n",
    "#     return outer_loss.item()\n",
    "\n",
    "# # Define the preference loss function\n",
    "# def preference_loss(reward_net, preference_predictor, task_data):\n",
    "#     loss = 0\n",
    "#     for task in task_data:\n",
    "#         states1, actions1, states2, actions2, labels = task\n",
    "#         predicted_preferences = preference_predictor(states1, actions1, states2, actions2)\n",
    "#         loss += -torch.mean(labels * torch.log(predicted_preferences + 1e-8) + (1 - labels) * torch.log(1 - predicted_preferences + 1e-8))\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# # Parameters\n",
    "# state_dim = 39\n",
    "# action_dim = 4\n",
    "# batch_size = 32\n",
    "# num_inner_updates = 5\n",
    "# alpha = 0.01  # Inner learning rate\n",
    "# beta = 0.001  # Outer learning rate\n",
    "# num_tasks = 100\n",
    "# num_epochs = 10\n",
    "\n",
    "# # Initialize reward function and preference predictor\n",
    "# reward_net = RewardFunction(state_dim, action_dim)\n",
    "# preference_predictor = PreferencePredictor(state_dim, action_dim)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0\n",
    "#     for episode in episodes:\n",
    "#         # Sample task data\n",
    "        \n",
    "#         # MAML update\n",
    "#         loss = maml_update(reward_net, preference_predictor, episode, alpha, beta)\n",
    "#         total_loss += loss\n",
    "    \n",
    "#     avg_loss = total_loss / num_tasks\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Define your neural network model\n",
    "# class Net(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "#         super(Net, self).__init__()\n",
    "#         # Define your network architecture\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.relu1(self.fc1(x))\n",
    "#         x = self.relu2(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define MAML class\n",
    "# class MAML:\n",
    "#     def __init__(self, model, lr_inner=0.01, lr_outer=0.001):\n",
    "#         self.model = model\n",
    "#         self.lr_inner = lr_inner\n",
    "#         self.lr_outer = lr_outer\n",
    "#         self.optimizer_outer = optim.Adam(self.model.parameters(), lr=lr_outer)\n",
    "        \n",
    "#     def inner_loop(self, S1, S2, y):\n",
    "#         # Inner loop training (fine-tuning)\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer_inner = optim.SGD(self.model.parameters(), lr=self.lr_inner)\n",
    "        \n",
    "#         # Convert DataFrames to tensors\n",
    "#         S1_input = torch.tensor(S1[[0, 1, 2]].values, dtype=torch.float32)\n",
    "#         S2_input = torch.tensor(S2[[0, 1, 2]].values, dtype=torch.float32)\n",
    "#         y_tensor = torch.tensor(y)\n",
    "        \n",
    "#         for _ in range(num_inner_updates=5):\n",
    "#             # Concatenate S1 and S2 tensors\n",
    "#             x = torch.cat([S1_input, S2_input], dim=0)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             logits = self.model(x)\n",
    "            \n",
    "#             # Compute loss\n",
    "#             loss = criterion(logits, y_tensor)\n",
    "            \n",
    "#             # Backward pass\n",
    "#             optimizer_inner.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer_inner.step()\n",
    "    \n",
    "#     def outer_loop(self, episodes, num_inner_updates=5):\n",
    "#         # Outer loop meta-training\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         for episode in episodes:\n",
    "#             for S1, S2, y, _ in episode:\n",
    "#                 # Inner loop (fine-tuning)\n",
    "#                 self.inner_loop(S1, S2, y)\n",
    "                \n",
    "#                 # Convert DataFrames to tensors\n",
    "#                 S1_input = torch.tensor(S1[[0, 1, 2]].values, dtype=torch.float32)\n",
    "#                 S2_input = torch.tensor(S2[[0, 1, 2]].values, dtype=torch.float32)\n",
    "#                 y_tensor = torch.tensor(y)\n",
    "                \n",
    "#                 # Concatenate S1 and S2 tensors\n",
    "#                 x = torch.cat([S1_input, S2_input], dim=0)\n",
    "                \n",
    "#                 # Compute loss and update outer loop parameters\n",
    "#                 logits = self.model(x)\n",
    "#                 loss = criterion(logits, y_tensor)\n",
    "#                 self.optimizer_outer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer_outer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# input_size =  39# Define your input size\n",
    "# hidden_size1 = 64  # Define your first hidden size\n",
    "# hidden_size2 = 32  # Define your second hidden size\n",
    "# output_size = 2  # Since you have binary classification\n",
    "# model = Net(input_size, hidden_size1, hidden_size2, output_size)\n",
    "# maml = MAML(model)\n",
    "\n",
    "# # Assuming you have episodes as described in your scenario\n",
    "# # Define your episodes\n",
    "# num_inner_updates = 5  # Number of inner loop updates\n",
    "# maml.outer_loop(episodes, num_inner_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example testing\n",
    "# def test(model, test_data):\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for S1, S2, y, _ in test_data:\n",
    "#             x = torch.cat([S1, S2], dim=0)  # Assuming S1 and S2 are tensors\n",
    "#             y = torch.tensor(y)\n",
    "#             outputs = model(x)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += y.size(0)\n",
    "#             correct += (predicted == y).sum().item()\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print('Accuracy of the network on the test data: %d %%' % accuracy)\n",
    "\n",
    "# # Assuming you have test data\n",
    "# test_data = [...]  # Define your test data\n",
    "# test(model, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
